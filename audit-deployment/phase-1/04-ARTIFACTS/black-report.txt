--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/__init__.py	2025-09-05 00:20:13.553408+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/__init__.py	2025-10-17 00:12:38.061209+00:00
@@ -25,7 +25,7 @@
     "Robustness",
     "Plotter",
     "ArtifactStore",
     "HandoffLedger",
     "BudgetGuard",
-    "PII"
+    "PII",
 ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/__init__.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/config.py	2025-09-04 23:51:40.971635+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/config.py	2025-10-17 00:12:38.079809+00:00
@@ -8,79 +8,91 @@
 from pydantic import BaseModel, Field, validator
 
 
 class DataConfig(BaseModel):
     """Data source configuration."""
+
     source: Literal["builtin", "csv"] = "builtin"
     name: str = "iris"  # iris | breast_cancer | diabetes | csv
     csv_paths: List[str] = Field(default_factory=list)
     target: str = ""  # auto for builtins; required for csv
 
 
 class BudgetConfig(BaseModel):
     """Resource budget limits."""
+
     time_min: int = 25
     memory_gb: float = 4.0
     token_budget: int = 8000
 
 
 class FairnessConfig(BaseModel):
     """Fairness evaluation settings."""
+
     enabled: bool = False
     sensitive_cols: List[str] = Field(default_factory=list)
     policy: Literal["report", "block"] = "report"
 
 
 class PIIConfig(BaseModel):
     """PII detection and handling."""
+
     enabled: bool = True
     patterns: List[str] = Field(default_factory=lambda: ["email", "phone"])
     action: Literal["redact", "block"] = "redact"
 
 
 class SplitConfig(BaseModel):
     """Data splitting configuration."""
+
     policy: Literal["iid", "time"] = "iid"
     time_col: str = ""
     test_size: float = 0.2
     val_size: float = 0.2
     rolling: bool = False
     seed: int = 42
 
 
 class SamplingConfig(BaseModel):
     """Sampling configuration for large datasets."""
+
     eda_rows: int = 200000
     stratify_by: List[str] = Field(default_factory=list)
 
 
 class CacheConfig(BaseModel):
     """Cache behavior settings."""
+
     mode: Literal["warm", "cold", "paranoid"] = "warm"
     dir: str = "./cache"
 
 
 class ReportConfig(BaseModel):
     """Report generation settings."""
+
     out_dir: str = "./artifacts"
     format: Literal["html", "md"] = "html"
 
 
 class LLMConfig(BaseModel):
     """LLM configuration."""
+
     openai_model: str = "gpt-4o-mini"
     ollama_model: str = "llama3.2"
 
 
 class Config(BaseModel):
     """Main configuration model."""
+
     data: DataConfig = Field(default_factory=DataConfig)
     task: Literal["auto", "classification", "regression"] = "auto"
-    metrics: Dict[str, Any] = Field(default_factory=lambda: {
-        "primary": "auto",
-        "secondary": ["roc_auc", "accuracy", "rmse"]
-    })
+    metrics: Dict[str, Any] = Field(
+        default_factory=lambda: {
+            "primary": "auto",
+            "secondary": ["roc_auc", "accuracy", "rmse"],
+        }
+    )
     business_goal: str = "Plain English goal"
     budgets: BudgetConfig = Field(default_factory=BudgetConfig)
     fairness: FairnessConfig = Field(default_factory=FairnessConfig)
     pii: PIIConfig = Field(default_factory=PIIConfig)
     split: SplitConfig = Field(default_factory=SplitConfig)
@@ -102,14 +114,14 @@
     def from_yaml(cls, path: str) -> "Config":
         """Load configuration from YAML file."""
         config_path = Path(path)
         if not config_path.exists():
             raise FileNotFoundError(f"Config file not found: {path}")
-        
+
         with open(config_path, "r") as f:
             data = yaml.safe_load(f)
-        
+
         return cls(**data)
 
     def validate_environment(self) -> None:
         """Validate required environment variables."""
         if not os.getenv("OPENAI_API_KEY"):
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/config.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/llm.py	2025-09-04 23:53:10.579942+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/llm.py	2025-10-17 00:12:38.080335+00:00
@@ -10,159 +10,151 @@
 from .context import DecisionLog
 
 
 class LLMRouter:
     """Routes requests to appropriate LLM based on criticality."""
-    
+
     def __init__(self, config: Dict[str, Any], decision_log: DecisionLog):
         self.config = config
         self.decision_log = decision_log
-        
+
         # Initialize OpenAI client (required)
         api_key = os.getenv("OPENAI_API_KEY")
         if not api_key:
             raise ValueError(
                 "OPENAI_API_KEY environment variable is required. "
                 "OpenAI is the sole decision authority - no fallback available."
             )
-        
+
         self.openai_client = OpenAI(api_key=api_key)
         self.openai_model = config.get("openai_model", "gpt-4o-mini")
-        
+
         # Ollama settings (optional)
         self.ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
         self.ollama_model = config.get("ollama_model", "llama3.2")
         self.ollama_available = self._check_ollama_availability()
-    
+
     def _check_ollama_availability(self) -> bool:
         """Check if Ollama is available."""
         try:
             response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
             return response.status_code == 200
         except Exception:
             return False
-    
+
     def openai_decide(
         self,
         stage: str,
         prompt: str,
         tools: Optional[List[Dict[str, Any]]] = None,
-        context: Optional[Dict[str, Any]] = None
+        context: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, Any]:
         """Make a critical decision using OpenAI with function calling."""
         messages = [
             {
                 "role": "system",
                 "content": (
                     "You are a senior data scientist making critical decisions. "
                     "Be precise, evidence-based, and explain your reasoning clearly. "
                     "Use the provided tools to gather information before deciding."
-                )
+                ),
             },
-            {"role": "user", "content": prompt}
+            {"role": "user", "content": prompt},
         ]
-        
+
         if context:
-            messages.insert(1, {
-                "role": "system", 
-                "content": f"Context: {json.dumps(context, indent=2)}"
-            })
-        
+            messages.insert(
+                1,
+                {
+                    "role": "system",
+                    "content": f"Context: {json.dumps(context, indent=2)}",
+                },
+            )
+
         try:
             if tools:
                 response = self.openai_client.chat.completions.create(
                     model=self.openai_model,
                     messages=messages,
                     tools=tools,
                     tool_choice="auto",
-                    temperature=0.1  # Low temperature for consistent decisions
+                    temperature=0.1,  # Low temperature for consistent decisions
                 )
             else:
                 response = self.openai_client.chat.completions.create(
-                    model=self.openai_model,
-                    messages=messages,
-                    temperature=0.1
+                    model=self.openai_model, messages=messages, temperature=0.1
                 )
-            
+
             # Extract decision and rationale
             message = response.choices[0].message
             decision_text = message.content or ""
-            
+
             # Handle tool calls if present
             tool_calls = []
             if message.tool_calls:
                 tool_calls = [
                     {
                         "name": call.function.name,
                         "arguments": json.loads(call.function.arguments),
-                        "id": call.id
+                        "id": call.id,
                     }
                     for call in message.tool_calls
                 ]
-            
+
             result = {
                 "decision": decision_text,
                 "tool_calls": tool_calls,
                 "model": self.openai_model,
                 "usage": {
                     "prompt_tokens": response.usage.prompt_tokens,
                     "completion_tokens": response.usage.completion_tokens,
-                    "total_tokens": response.usage.total_tokens
-                }
+                    "total_tokens": response.usage.total_tokens,
+                },
             }
-            
+
             # Log the decision
             self.decision_log.record_decision(
                 stage=stage,
                 decision=decision_text,
                 rationale=f"OpenAI decision with {len(tool_calls)} tool calls",
                 inputs_refs=[f"prompt:{len(prompt)} chars"],
-                auth_model="openai"
+                auth_model="openai",
             )
-            
+
             return result
-            
+
         except Exception as e:
             raise RuntimeError(f"OpenAI decision failed: {str(e)}")
-    
-    def ollama_draft(
-        self,
-        prompt: str,
-        max_tokens: int = 500
-    ) -> Optional[str]:
+
+    def ollama_draft(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
         """Generate non-critical draft using Ollama (optional)."""
         if not self.ollama_available:
             return None
-        
+
         try:
             payload = {
                 "model": self.ollama_model,
                 "prompt": prompt,
                 "stream": False,
-                "options": {
-                    "num_predict": max_tokens,
-                    "temperature": 0.7
-                }
+                "options": {"num_predict": max_tokens, "temperature": 0.7},
             }
-            
+
             response = requests.post(
-                f"{self.ollama_base_url}/api/generate",
-                json=payload,
-                timeout=30
+                f"{self.ollama_base_url}/api/generate", json=payload, timeout=30
             )
-            
+
             if response.status_code == 200:
                 return response.json().get("response", "")
             else:
                 return None
-                
+
         except Exception:
             return None
-    
+
     def get_usage_stats(self) -> Dict[str, Any]:
         """Get usage statistics."""
         return {
             "openai_model": self.openai_model,
             "ollama_model": self.ollama_model,
             "ollama_available": self.ollama_available,
-            "decisions_logged": len(self.decision_log.get_decisions())
+            "decisions_logged": len(self.decision_log.get_decisions()),
         }
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/llm.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/data_io.py	2025-09-04 23:53:37.611077+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/data_io.py	2025-10-17 00:12:38.091076+00:00
@@ -9,106 +9,107 @@
 from ..utils import ensure_dir, hash_dataset, save_pickle
 
 
 class DataStore:
     """Data loading and management tools."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.data_dir = ensure_dir(artifacts_dir / "data")
-    
+
     def read_builtin(self, name: str) -> Dict[str, Any]:
         """
         Load a built-in sklearn dataset.
-        
+
         Args:
             name: Dataset name (iris, breast_cancer, diabetes)
-            
+
         Returns:
             Dict with df_ref, target, task_hint
         """
         if name == "iris":
             data = datasets.load_iris()
             df = pd.DataFrame(data.data, columns=data.feature_names)
-            df['target'] = data.target
-            target = 'target'
-            task_hint = 'classification'
-            
+            df["target"] = data.target
+            target = "target"
+            task_hint = "classification"
+
         elif name == "breast_cancer":
             data = datasets.load_breast_cancer()
             df = pd.DataFrame(data.data, columns=data.feature_names)
-            df['target'] = data.target
-            target = 'target'
-            task_hint = 'classification'
-            
+            df["target"] = data.target
+            target = "target"
+            task_hint = "classification"
+
         elif name == "diabetes":
             data = datasets.load_diabetes()
             df = pd.DataFrame(data.data, columns=data.feature_names)
-            df['target'] = data.target
-            target = 'target'
-            task_hint = 'regression'
-            
+            df["target"] = data.target
+            target = "target"
+            task_hint = "regression"
+
         else:
             raise ValueError(f"Unknown builtin dataset: {name}")
-        
+
         # Save dataset
         dataset_hash = hash_dataset(df, name)
         df_path = self.data_dir / f"{name}_{dataset_hash[:8]}.pkl"
         save_pickle(df, df_path)
-        
+
         return {
             "df_ref": str(df_path),
             "target": target,
             "task_hint": task_hint,
             "shape": df.shape,
-            "hash": dataset_hash
+            "hash": dataset_hash,
         }
-    
+
     def read_csv(self, paths: List[str]) -> Dict[str, Any]:
         """
         Load CSV files and combine if multiple.
-        
+
         Args:
             paths: List of CSV file paths
-            
+
         Returns:
             Dict with df_ref
         """
         if not paths:
             raise ValueError("No CSV paths provided")
-        
+
         dfs = []
         for path in paths:
             csv_path = Path(path)
             if not csv_path.exists():
                 raise FileNotFoundError(f"CSV file not found: {path}")
-            
+
             df = pd.read_csv(csv_path)
             dfs.append(df)
-        
+
         # Combine if multiple files
         if len(dfs) == 1:
             combined_df = dfs[0]
         else:
             combined_df = pd.concat(dfs, ignore_index=True)
-        
+
         # Save combined dataset
         dataset_hash = hash_dataset(combined_df, "csv")
         df_path = self.data_dir / f"csv_{dataset_hash[:8]}.pkl"
         save_pickle(combined_df, df_path)
-        
+
         return {
             "df_ref": str(df_path),
             "shape": combined_df.shape,
-            "hash": dataset_hash
+            "hash": dataset_hash,
         }
-    
+
     def load_dataframe(self, df_ref: str) -> pd.DataFrame:
         """Load DataFrame from reference."""
         from ..utils import load_pickle
+
         return load_pickle(df_ref)
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions for this tool."""
         return [
             {
                 "type": "function",
@@ -119,31 +120,31 @@
                         "type": "object",
                         "properties": {
                             "name": {
                                 "type": "string",
                                 "enum": ["iris", "breast_cancer", "diabetes"],
-                                "description": "Name of the built-in dataset"
+                                "description": "Name of the built-in dataset",
                             }
                         },
-                        "required": ["name"]
-                    }
-                }
+                        "required": ["name"],
+                    },
+                },
             },
             {
-                "type": "function", 
+                "type": "function",
                 "function": {
                     "name": "DataStore_read_csv",
                     "description": "Load CSV files and combine if multiple",
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "paths": {
                                 "type": "array",
                                 "items": {"type": "string"},
-                                "description": "List of CSV file paths to load"
+                                "description": "List of CSV file paths to load",
                             }
                         },
-                        "required": ["paths"]
-                    }
-                }
-            }
+                        "required": ["paths"],
+                    },
+                },
+            },
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/data_io.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/context.py	2025-09-04 23:52:22.478235+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/context.py	2025-10-17 00:12:38.096273+00:00
@@ -7,178 +7,178 @@
 from .utils import get_timestamp, save_json
 
 
 class DecisionLog:
     """Log for critical decisions made by OpenAI."""
-    
+
     def __init__(self, log_path: Path):
         self.log_path = log_path
         self.log_path.parent.mkdir(parents=True, exist_ok=True)
-    
+
     def record_decision(
         self,
         stage: str,
         decision: str,
         rationale: str,
         inputs_refs: List[str],
-        auth_model: str = "openai"
+        auth_model: str = "openai",
     ) -> None:
         """Record a critical decision."""
         entry = {
             "stage": stage,
             "decision": decision,
             "rationale": rationale,
             "inputs_refs": inputs_refs,
             "timestamp": get_timestamp(),
-            "auth_model": auth_model
+            "auth_model": auth_model,
         }
-        
+
         # Append to JSONL file
         with open(self.log_path, "a") as f:
             f.write(json.dumps(entry) + "\n")
-    
+
     def get_decisions(self) -> List[Dict[str, Any]]:
         """Get all recorded decisions."""
         if not self.log_path.exists():
             return []
-        
+
         decisions = []
         with open(self.log_path, "r") as f:
             for line in f:
                 if line.strip():
                     decisions.append(json.loads(line))
         return decisions
 
 
 class DataCard:
     """Data card for dataset documentation."""
-    
+
     def __init__(self, name: str, version: str = "v1"):
         self.name = name
         self.version = version
         self.created_at = get_timestamp()
         self.metadata: Dict[str, Any] = {}
         self.quality_notes: List[str] = []
         self.leakage_rules: List[str] = []
-    
+
     def add_metadata(self, key: str, value: Any) -> None:
         """Add metadata entry."""
         self.metadata[key] = value
-    
+
     def add_quality_note(self, note: str) -> None:
         """Add quality observation."""
         self.quality_notes.append(note)
-    
+
     def add_leakage_rule(self, rule: str) -> None:
         """Add leakage prevention rule."""
         self.leakage_rules.append(rule)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary."""
         return {
             "name": self.name,
             "version": self.version,
             "created_at": self.created_at,
             "metadata": self.metadata,
             "quality_notes": self.quality_notes,
-            "leakage_rules": self.leakage_rules
+            "leakage_rules": self.leakage_rules,
         }
-    
+
     def save(self, path: Path) -> str:
         """Save data card and return hash."""
         return save_json(self.to_dict(), path)
 
 
 class HandoffLedger:
     """Ledger for tracking handoffs between pipeline stages."""
-    
+
     def __init__(self, ledger_path: Path):
         self.ledger_path = ledger_path
         self.ledger_path.parent.mkdir(parents=True, exist_ok=True)
         self.entries: List[Dict[str, Any]] = []
-    
+
     def append(
         self,
         job_id: str,
         stage: str,
         input_refs: List[str],
         output_refs: List[str],
         schema_uri: str,
-        hash_value: str
+        hash_value: str,
     ) -> None:
         """Append a handoff entry."""
         entry = {
             "job_id": job_id,
             "stage": stage,
             "inputs": input_refs,
             "outputs": output_refs,
             "schema": schema_uri,
             "hash": hash_value,
-            "timestamp": get_timestamp()
+            "timestamp": get_timestamp(),
         }
-        
+
         self.entries.append(entry)
-        
+
         # Append to JSONL file
         with open(self.ledger_path, "a") as f:
             f.write(json.dumps(entry) + "\n")
-    
+
     def get_entries(self) -> List[Dict[str, Any]]:
         """Get all ledger entries."""
         if not self.ledger_path.exists():
             return []
-        
+
         entries = []
         with open(self.ledger_path, "r") as f:
             for line in f:
                 if line.strip():
                     entries.append(json.loads(line))
         return entries
 
 
 class RunManifest:
     """Manifest for a complete pipeline run."""
-    
+
     def __init__(self, job_id: str):
         self.job_id = job_id
         self.created_at = get_timestamp()
         self.config_hash: Optional[str] = None
         self.dataset_hash: Optional[str] = None
         self.seeds: Dict[str, int] = {}
         self.budgets_used: Dict[str, Any] = {}
         self.cache_hits: Dict[str, bool] = {}
         self.versions: Dict[str, str] = {}
         self.shortcuts_taken: List[str] = []
-    
+
     def set_config_hash(self, hash_value: str) -> None:
         """Set configuration hash."""
         self.config_hash = hash_value
-    
+
     def set_dataset_hash(self, hash_value: str) -> None:
         """Set dataset hash."""
         self.dataset_hash = hash_value
-    
+
     def add_seed(self, component: str, seed: int) -> None:
         """Add seed used by a component."""
         self.seeds[component] = seed
-    
+
     def add_budget_usage(self, stage: str, usage: Dict[str, Any]) -> None:
         """Add budget usage for a stage."""
         self.budgets_used[stage] = usage
-    
+
     def add_cache_hit(self, stage: str, hit: bool) -> None:
         """Record cache hit/miss for a stage."""
         self.cache_hits[stage] = hit
-    
+
     def add_version(self, component: str, version: str) -> None:
         """Add version information."""
         self.versions[component] = version
-    
+
     def add_shortcut(self, description: str) -> None:
         """Add shortcut taken due to budget constraints."""
         self.shortcuts_taken.append(description)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary."""
         return {
             "job_id": self.job_id,
             "created_at": self.created_at,
@@ -186,11 +186,11 @@
             "dataset_hash": self.dataset_hash,
             "seeds": self.seeds,
             "budgets_used": self.budgets_used,
             "cache_hits": self.cache_hits,
             "versions": self.versions,
-            "shortcuts_taken": self.shortcuts_taken
+            "shortcuts_taken": self.shortcuts_taken,
         }
-    
+
     def save(self, path: Path) -> str:
         """Save manifest and return hash."""
         return save_json(self.to_dict(), path)
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/context.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/budget.py	2025-09-04 23:59:47.893321+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/budget.py	2025-10-17 00:12:38.106684+00:00
@@ -7,164 +7,174 @@
 from ..utils import get_memory_usage_gb, get_timestamp
 
 
 class BudgetGuard:
     """Budget monitoring and enforcement."""
-    
+
     def __init__(self, artifacts_dir: Path, budgets: Dict[str, Any]):
         self.artifacts_dir = artifacts_dir
         self.budgets = budgets
         self.start_time = time.time()
         self.checkpoints: List[Dict[str, Any]] = []
         self.token_usage = 0
-        
+
         # Budget limits
         self.time_limit_seconds = budgets.get("time_min", 25) * 60
         self.memory_limit_gb = budgets.get("memory_gb", 4.0)
         self.token_limit = budgets.get("token_budget", 8000)
-    
-    def checkpoint(
-        self,
-        stage: str,
-        additional_tokens: int = 0
-    ) -> Dict[str, Any]:
+
+    def checkpoint(self, stage: str, additional_tokens: int = 0) -> Dict[str, Any]:
         """
         Check budget status at a pipeline stage.
-        
+
         Args:
             stage: Current pipeline stage
             additional_tokens: Additional tokens used in this stage
-            
+
         Returns:
             Budget status and recommendations
         """
         current_time = time.time()
         elapsed_time = current_time - self.start_time
         current_memory = get_memory_usage_gb()
         self.token_usage += additional_tokens
-        
+
         # Create checkpoint record
         checkpoint = {
             "stage": stage,
             "timestamp": get_timestamp(),
             "elapsed_seconds": elapsed_time,
             "memory_gb": current_memory,
             "tokens_used": self.token_usage,
             "budgets": {
-                "time_remaining_seconds": max(0, self.time_limit_seconds - elapsed_time),
+                "time_remaining_seconds": max(
+                    0, self.time_limit_seconds - elapsed_time
+                ),
                 "memory_remaining_gb": max(0, self.memory_limit_gb - current_memory),
-                "tokens_remaining": max(0, self.token_limit - self.token_usage)
-            }
+                "tokens_remaining": max(0, self.token_limit - self.token_usage),
+            },
         }
-        
+
         self.checkpoints.append(checkpoint)
-        
+
         # Determine status and recommendations
         status = "ok"
         recommendations = []
-        
+
         # Time budget check
         time_usage_pct = elapsed_time / self.time_limit_seconds
         if time_usage_pct > 0.9:
             status = "abort"
             recommendations.append("Time budget exceeded - consider aborting")
         elif time_usage_pct > 0.7:
             status = "downshift"
-            recommendations.append("Time budget tight - consider reducing model complexity")
-        
+            recommendations.append(
+                "Time budget tight - consider reducing model complexity"
+            )
+
         # Memory budget check
         memory_usage_pct = current_memory / self.memory_limit_gb
         if memory_usage_pct > 0.9:
             status = "abort"
             recommendations.append("Memory budget exceeded - consider smaller dataset")
         elif memory_usage_pct > 0.7:
             status = "downshift"
             recommendations.append("Memory usage high - consider feature reduction")
-        
+
         # Token budget check
         token_usage_pct = self.token_usage / self.token_limit
         if token_usage_pct > 0.9:
             status = "abort"
             recommendations.append("Token budget exceeded - reduce LLM calls")
         elif token_usage_pct > 0.7:
             status = "downshift"
             recommendations.append("Token usage high - simplify remaining decisions")
-        
+
         # Overall status (most restrictive wins)
         if any("abort" in rec for rec in recommendations):
             status = "abort"
         elif any("downshift" in rec for rec in recommendations):
             status = "downshift"
-        
+
         return {
             "status": status,
             "stage": stage,
             "usage": {
                 "time_pct": round(time_usage_pct * 100, 1),
                 "memory_pct": round(memory_usage_pct * 100, 1),
-                "tokens_pct": round(token_usage_pct * 100, 1)
+                "tokens_pct": round(token_usage_pct * 100, 1),
             },
             "recommendations": recommendations,
-            "checkpoint": checkpoint
+            "checkpoint": checkpoint,
         }
-    
+
     def get_usage_summary(self) -> Dict[str, Any]:
         """Get overall budget usage summary."""
         if not self.checkpoints:
             return {"status": "no_checkpoints"}
-        
+
         latest = self.checkpoints[-1]
-        
+
         return {
             "total_stages": len(self.checkpoints),
             "final_usage": {
                 "time_seconds": latest["elapsed_seconds"],
-                "time_pct": round(latest["elapsed_seconds"] / self.time_limit_seconds * 100, 1),
+                "time_pct": round(
+                    latest["elapsed_seconds"] / self.time_limit_seconds * 100, 1
+                ),
                 "memory_gb": latest["memory_gb"],
-                "memory_pct": round(latest["memory_gb"] / self.memory_limit_gb * 100, 1),
+                "memory_pct": round(
+                    latest["memory_gb"] / self.memory_limit_gb * 100, 1
+                ),
                 "tokens": latest["tokens_used"],
-                "tokens_pct": round(latest["tokens_used"] / self.token_limit * 100, 1)
+                "tokens_pct": round(latest["tokens_used"] / self.token_limit * 100, 1),
             },
-            "checkpoints": self.checkpoints
+            "checkpoints": self.checkpoints,
         }
-    
+
     def suggest_shortcuts(self, remaining_stages: List[str]) -> List[str]:
         """Suggest shortcuts for remaining stages based on budget constraints."""
         shortcuts = []
-        
+
         latest_checkpoint = self.checkpoints[-1] if self.checkpoints else None
         if not latest_checkpoint:
             return shortcuts
-        
+
         usage = latest_checkpoint["budgets"]
-        
+
         # Time-based shortcuts
         if usage["time_remaining_seconds"] < 300:  # Less than 5 minutes
-            shortcuts.extend([
-                "Skip hyperparameter tuning - use default parameters",
-                "Reduce cross-validation folds from 5 to 3",
-                "Skip robustness testing - use basic evaluation only"
-            ])
-        
+            shortcuts.extend(
+                [
+                    "Skip hyperparameter tuning - use default parameters",
+                    "Reduce cross-validation folds from 5 to 3",
+                    "Skip robustness testing - use basic evaluation only",
+                ]
+            )
+
         # Memory-based shortcuts
         if usage["memory_remaining_gb"] < 1.0:  # Less than 1GB
-            shortcuts.extend([
-                "Sample dataset to 50% for remaining stages",
-                "Use simpler models (linear instead of tree-based)",
-                "Skip feature importance analysis"
-            ])
-        
+            shortcuts.extend(
+                [
+                    "Sample dataset to 50% for remaining stages",
+                    "Use simpler models (linear instead of tree-based)",
+                    "Skip feature importance analysis",
+                ]
+            )
+
         # Token-based shortcuts
         if usage["tokens_remaining"] < 1000:  # Less than 1000 tokens
-            shortcuts.extend([
-                "Use cached decisions where possible",
-                "Simplify report generation - basic template only",
-                "Skip detailed explanations in decision log"
-            ])
-        
+            shortcuts.extend(
+                [
+                    "Use cached decisions where possible",
+                    "Simplify report generation - basic template only",
+                    "Skip detailed explanations in decision log",
+                ]
+            )
+
         return shortcuts
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -174,17 +184,17 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "stage": {
                                 "type": "string",
-                                "description": "Current pipeline stage name"
+                                "description": "Current pipeline stage name",
                             },
                             "additional_tokens": {
                                 "type": "integer",
-                                "description": "Additional tokens used in this stage (default: 0)"
-                            }
+                                "description": "Additional tokens used in this stage (default: 0)",
+                            },
                         },
-                        "required": ["stage"]
-                    }
-                }
+                        "required": ["stage"],
+                    },
+                },
             }
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/budget.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/cache.py	2025-09-04 23:52:45.833585+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/cache.py	2025-10-17 00:12:38.109812+00:00
@@ -2,184 +2,184 @@
 
 import json
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Set
 
-from .utils import ensure_dir, hash_object, load_json, load_pickle, save_json, save_pickle
+from .utils import (
+    ensure_dir,
+    hash_object,
+    load_json,
+    load_pickle,
+    save_json,
+    save_pickle,
+)
 
 
 class CacheIndex:
     """Index for tracking cache entries and dependencies."""
-    
+
     def __init__(self, cache_dir: Path):
         self.cache_dir = ensure_dir(cache_dir)
         self.index_path = self.cache_dir / "index.json"
         self.index: Dict[str, Dict[str, Any]] = self._load_index()
-    
+
     def _load_index(self) -> Dict[str, Dict[str, Any]]:
         """Load cache index from disk."""
         if self.index_path.exists():
             return load_json(self.index_path)
         return {}
-    
+
     def _save_index(self) -> None:
         """Save cache index to disk."""
         save_json(self.index, self.index_path)
-    
+
     def get_entry(self, key: str) -> Optional[Dict[str, Any]]:
         """Get cache entry by key."""
         return self.index.get(key)
-    
+
     def put_entry(
         self,
         key: str,
         file_path: str,
         dependencies: List[str],
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> None:
         """Add cache entry."""
         self.index[key] = {
             "file_path": file_path,
             "dependencies": dependencies,
             "metadata": metadata or {},
-            "created_at": hash_object("timestamp")  # Simple timestamp
+            "created_at": hash_object("timestamp"),  # Simple timestamp
         }
         self._save_index()
-    
+
     def invalidate_key(self, key: str) -> None:
         """Remove cache entry."""
         if key in self.index:
             # Remove file if it exists
             entry = self.index[key]
             file_path = Path(entry["file_path"])
             if file_path.exists():
                 file_path.unlink()
-            
+
             del self.index[key]
             self._save_index()
-    
+
     def invalidate_dependents(self, changed_key: str) -> Set[str]:
         """Invalidate all entries that depend on changed_key."""
         invalidated = set()
-        
+
         for key, entry in list(self.index.items()):
             if changed_key in entry.get("dependencies", []):
                 self.invalidate_key(key)
                 invalidated.add(key)
-        
+
         return invalidated
-    
+
     def get_stats(self) -> Dict[str, Any]:
         """Get cache statistics."""
         return {
             "total_entries": len(self.index),
             "cache_dir": str(self.cache_dir),
-            "index_path": str(self.index_path)
+            "index_path": str(self.index_path),
         }
 
 
 class Cache:
     """Main cache interface."""
-    
+
     def __init__(self, cache_dir: str, mode: str = "warm"):
         self.cache_dir = Path(cache_dir)
         self.mode = mode  # warm, cold, paranoid
         self.index = CacheIndex(self.cache_dir)
         self.hits: Dict[str, bool] = {}
-    
+
     def _get_cache_path(self, stage: str, key: str) -> Path:
         """Get cache file path for stage and key."""
         return self.cache_dir / stage / f"{key}.pkl"
-    
+
     def get(self, stage: str, key: str) -> Optional[Any]:
         """Get cached object."""
         if self.mode == "cold":
             self.hits[f"{stage}:{key}"] = False
             return None
-        
+
         cache_key = f"{stage}:{key}"
         entry = self.index.get_entry(cache_key)
-        
+
         if entry is None:
             self.hits[cache_key] = False
             return None
-        
+
         cache_path = Path(entry["file_path"])
         if not cache_path.exists():
             self.index.invalidate_key(cache_key)
             self.hits[cache_key] = False
             return None
-        
+
         try:
             obj = load_pickle(cache_path)
             self.hits[cache_key] = True
             return obj
         except Exception:
             self.index.invalidate_key(cache_key)
             self.hits[cache_key] = False
             return None
-    
+
     def put(
-        self,
-        stage: str,
-        key: str,
-        obj: Any,
-        dependencies: Optional[List[str]] = None
+        self, stage: str, key: str, obj: Any, dependencies: Optional[List[str]] = None
     ) -> None:
         """Cache an object."""
         if self.mode == "cold":
             return
-        
+
         cache_path = self._get_cache_path(stage, key)
         ensure_dir(cache_path.parent)
-        
+
         # Save object
         save_pickle(obj, cache_path)
-        
+
         # Update index
         cache_key = f"{stage}:{key}"
         self.index.put_entry(
-            cache_key,
-            str(cache_path),
-            dependencies or [],
-            {"stage": stage, "key": key}
+            cache_key, str(cache_path), dependencies or [], {"stage": stage, "key": key}
         )
-    
+
     def invalidate_stage(self, stage: str) -> None:
         """Invalidate all cache entries for a stage."""
         stage_keys = [k for k in self.index.index.keys() if k.startswith(f"{stage}:")]
         for key in stage_keys:
             self.index.invalidate_key(key)
-    
+
     def invalidate_downstream(self, changed_stage: str) -> Set[str]:
         """Invalidate stages downstream of changed stage."""
         # Define stage dependencies
         stage_deps = {
             "profile": [],
             "eda": ["profile"],
             "feature_plan": ["profile", "eda"],
             "split_indices": ["profile"],
             "ladder": ["feature_plan", "split_indices"],
             "evaluation": ["ladder"],
-            "reports": ["evaluation"]
+            "reports": ["evaluation"],
         }
-        
+
         invalidated = set()
-        
+
         # Find all stages that depend on changed_stage
         for stage, deps in stage_deps.items():
             if changed_stage in deps:
                 self.invalidate_stage(stage)
                 invalidated.add(stage)
                 # Recursively invalidate downstream
                 invalidated.update(self.invalidate_downstream(stage))
-        
+
         return invalidated
-    
+
     def get_hit_stats(self) -> Dict[str, bool]:
         """Get cache hit/miss statistics."""
         return self.hits.copy()
-    
+
     def clear_all(self) -> None:
         """Clear entire cache."""
         for key in list(self.index.index.keys()):
             self.index.invalidate_key(key)
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/cache.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/utils.py	2025-09-04 23:51:59.613628+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/utils.py	2025-10-17 00:12:38.152049+00:00
@@ -40,11 +40,11 @@
             content = obj.read_bytes()
         else:
             content = str(obj).encode()
     else:
         content = str(obj).encode()
-    
+
     return "sha256:" + hashlib.sha256(content).hexdigest()
 
 
 def hash_dataset(df: pd.DataFrame, name: str = "") -> str:
     """Generate a comprehensive hash for a dataset."""
@@ -69,14 +69,14 @@
 
 def save_json(obj: Any, path: Union[str, Path]) -> str:
     """Save object as JSON and return hash."""
     path = Path(path)
     ensure_dir(path.parent)
-    
+
     with open(path, "w") as f:
         json.dump(obj, f, indent=2, default=str)
-    
+
     return hash_object(obj)
 
 
 def load_json(path: Union[str, Path]) -> Any:
     """Load JSON from file."""
@@ -86,11 +86,11 @@
 
 def save_pickle(obj: Any, path: Union[str, Path]) -> str:
     """Save object as pickle and return hash."""
     path = Path(path)
     ensure_dir(path.parent)
-    
+
     joblib.dump(obj, path)
     return hash_object(obj)
 
 
 def load_pickle(path: Union[str, Path]) -> Any:
@@ -99,12 +99,13 @@
 
 
 def get_memory_usage_gb() -> float:
     """Get current memory usage in GB."""
     import psutil
+
     process = psutil.Process()
-    return process.memory_info().rss / (1024 ** 3)
+    return process.memory_info().rss / (1024**3)
 
 
 def format_duration(seconds: float) -> str:
     """Format duration in human-readable format."""
     if seconds < 60:
@@ -115,25 +116,25 @@
         return f"{seconds/3600:.1f}h"
 
 
 class Timer:
     """Context manager for timing operations."""
-    
+
     def __init__(self, name: str = "Operation"):
         self.name = name
         self.start_time = 0.0
         self.end_time = 0.0
-    
+
     def __enter__(self) -> "Timer":
         self.start_time = time.time()
         return self
-    
+
     def __exit__(self, *args: Any) -> None:
         self.end_time = time.time()
-    
+
     @property
     def elapsed(self) -> float:
         """Get elapsed time in seconds."""
         return self.end_time - self.start_time
-    
+
     def __str__(self) -> str:
         return f"{self.name}: {format_duration(self.elapsed)}"
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/utils.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/artifacts.py	2025-09-04 23:59:18.288348+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/artifacts.py	2025-10-17 00:12:38.159823+00:00
@@ -7,70 +7,67 @@
 from ..utils import ensure_dir, save_json
 
 
 class ArtifactStore:
     """Artifact storage and management."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.reports_dir = ensure_dir(artifacts_dir / "reports")
         self.tables_dir = ensure_dir(artifacts_dir / "tables")
         self.figures_dir = ensure_dir(artifacts_dir / "figures")
-    
+
     def write_report(
-        self,
-        kind: str,
-        payload: Dict[str, Any],
-        format: str = "html"
+        self, kind: str, payload: Dict[str, Any], format: str = "html"
     ) -> Dict[str, Any]:
         """
         Write report (one-pager or appendix).
-        
+
         Args:
             kind: 'one_pager' or 'appendix'
             payload: Report content payload
             format: 'html' or 'md'
-            
+
         Returns:
             Report file reference
         """
         if kind not in ["one_pager", "appendix"]:
             raise ValueError(f"Unknown report kind: {kind}")
-        
+
         if format == "html":
             report_content = self._generate_html_report(kind, payload)
             file_ext = "html"
         else:
             report_content = self._generate_markdown_report(kind, payload)
             file_ext = "md"
-        
+
         # Save report
         report_path = self.reports_dir / f"{kind}.{file_ext}"
         with open(report_path, "w") as f:
             f.write(report_content)
-        
+
         return {
             "report_ref": str(report_path),
             "kind": kind,
             "format": format,
-            "size_bytes": len(report_content)
+            "size_bytes": len(report_content),
         }
-    
+
     def _generate_html_report(self, kind: str, payload: Dict[str, Any]) -> str:
         """Generate HTML report content."""
         if kind == "one_pager":
             return self._generate_one_pager_html(payload)
         else:
             return self._generate_appendix_html(payload)
-    
+
     def _generate_markdown_report(self, kind: str, payload: Dict[str, Any]) -> str:
         """Generate Markdown report content."""
         if kind == "one_pager":
             return self._generate_one_pager_md(payload)
         else:
             return self._generate_appendix_md(payload)
-    
+
     def _generate_one_pager_html(self, payload: Dict[str, Any]) -> str:
         """Generate one-pager HTML."""
         html = f"""<!DOCTYPE html>
 <html>
 <head>
@@ -102,15 +99,15 @@
     </div>
     
     <h2>3. Top 3 Insights</h2>
     <ol>
 """
-        
-        insights = payload.get('top_insights', ['No insights available'] * 3)
+
+        insights = payload.get("top_insights", ["No insights available"] * 3)
         for insight in insights[:3]:
             html += f"        <li>{insight}</li>\n"
-        
+
         html += """    </ol>
     
     <h2>4. Model Decision</h2>
     <div class="success">
         <strong>Selected Model:</strong> {model_name}<br>
@@ -120,26 +117,28 @@
     
     <h3>Mini-Leaderboard (Top 3)</h3>
     <table>
         <tr><th>Rank</th><th>Model</th><th>Score</th><th>CI</th><th>Runtime</th></tr>
 """.format(
-            model_name=payload.get('selected_model', {}).get('name', 'Not selected'),
-            performance=payload.get('selected_model', {}).get('score', 'Not available'),
-            rationale=payload.get('selected_model', {}).get('rationale', 'Not provided')
+            model_name=payload.get("selected_model", {}).get("name", "Not selected"),
+            performance=payload.get("selected_model", {}).get("score", "Not available"),
+            rationale=payload.get("selected_model", {}).get(
+                "rationale", "Not provided"
+            ),
         )
-        
-        leaderboard = payload.get('leaderboard', [])
+
+        leaderboard = payload.get("leaderboard", [])
         for i, model in enumerate(leaderboard[:3], 1):
             html += f"""        <tr>
             <td>{i}</td>
             <td>{model.get('name', f'Model {i}')}</td>
             <td>{model.get('score', 'N/A')}</td>
             <td>{model.get('ci', 'N/A')}</td>
             <td>{model.get('runtime', 'N/A')}</td>
         </tr>
 """
-        
+
         html += f"""    </table>
     
     <h2>5. Operating Point</h2>
     <div class="metric">
         <strong>Recommended Threshold:</strong> {payload.get('operating_point', {}).get('threshold', 'Not set')}<br>
@@ -154,20 +153,20 @@
     </div>
     
     <h2>7. Next Steps</h2>
     <ul>
 """
-        
-        next_steps = payload.get('next_steps', ['No next steps defined'])
+
+        next_steps = payload.get("next_steps", ["No next steps defined"])
         for step in next_steps:
             html += f"        <li>{step}</li>\n"
-        
+
         html += """    </ul>
 """
-        
+
         # Add shortcuts section if any
-        shortcuts = payload.get('shortcuts_taken', [])
+        shortcuts = payload.get("shortcuts_taken", [])
         if shortcuts:
             html += """    
     <h2>8. Run Assumptions & Shortcuts</h2>
     <div class="warning">
         <ul>
@@ -175,17 +174,17 @@
             for shortcut in shortcuts:
                 html += f"            <li>{shortcut}</li>\n"
             html += """        </ul>
     </div>
 """
-        
+
         html += """
 </body>
 </html>"""
-        
+
         return html
-    
+
     def _generate_one_pager_md(self, payload: Dict[str, Any]) -> str:
         """Generate one-pager Markdown."""
         md = f"""# Executive One-Pager
 
 ## 1. Problem & Success Metric
@@ -200,15 +199,15 @@
 - **Leakage Status:** {payload.get('leakage_status', 'Not checked')}
 
 ## 3. Top 3 Insights
 
 """
-        
-        insights = payload.get('top_insights', ['No insights available'] * 3)
+
+        insights = payload.get("top_insights", ["No insights available"] * 3)
         for i, insight in enumerate(insights[:3], 1):
             md += f"{i}. {insight}\n"
-        
+
         md += f"""
 ## 4. Model Decision
 
 **Selected Model:** {payload.get('selected_model', {}).get('name', 'Not selected')}
 **Performance:** {payload.get('selected_model', {}).get('score', 'Not available')}
@@ -217,15 +216,15 @@
 ### Mini-Leaderboard (Top 3)
 
 | Rank | Model | Score | CI | Runtime |
 |------|-------|-------|----|---------| 
 """
-        
-        leaderboard = payload.get('leaderboard', [])
+
+        leaderboard = payload.get("leaderboard", [])
         for i, model in enumerate(leaderboard[:3], 1):
             md += f"| {i} | {model.get('name', f'Model {i}')} | {model.get('score', 'N/A')} | {model.get('ci', 'N/A')} | {model.get('runtime', 'N/A')} |\n"
-        
+
         md += f"""
 ## 5. Operating Point
 
 - **Recommended Threshold:** {payload.get('operating_point', {}).get('threshold', 'Not set')}
 - **Conservative Alternative:** {payload.get('operating_point', {}).get('conservative', 'Not set')}
@@ -237,24 +236,24 @@
 **Reason:** {payload.get('robustness_reason', 'Not provided')}
 
 ## 7. Next Steps
 
 """
-        
-        next_steps = payload.get('next_steps', ['No next steps defined'])
+
+        next_steps = payload.get("next_steps", ["No next steps defined"])
         for step in next_steps:
             md += f"- {step}\n"
-        
+
         # Add shortcuts section if any
-        shortcuts = payload.get('shortcuts_taken', [])
+        shortcuts = payload.get("shortcuts_taken", [])
         if shortcuts:
             md += "\n## 8. Run Assumptions & Shortcuts\n\n"
             for shortcut in shortcuts:
                 md += f"- {shortcut}\n"
-        
+
         return md
-    
+
     def _generate_appendix_html(self, payload: Dict[str, Any]) -> str:
         """Generate appendix HTML (simplified for MVP)."""
         return f"""<!DOCTYPE html>
 <html>
 <head>
@@ -279,11 +278,11 @@
         <p><strong>Seeds:</strong> {payload.get('seeds', 'Not available')}</p>
     </div>
     
 </body>
 </html>"""
-    
+
     def _generate_appendix_md(self, payload: Dict[str, Any]) -> str:
         """Generate appendix Markdown (simplified for MVP)."""
         return f"""# Technical Appendix
 
 ## Configuration
@@ -295,36 +294,31 @@
 ## Reproducibility
 
 - **Dataset Hash:** {payload.get('dataset_hash', 'Not available')}
 - **Seeds:** {payload.get('seeds', 'Not available')}
 """
-    
+
     def write_table(self, name: str, obj: Any) -> Dict[str, Any]:
         """Write table object to file."""
         table_path = self.tables_dir / f"{name}.json"
         hash_value = save_json(obj, table_path)
-        
-        return {
-            "table_ref": str(table_path),
-            "hash": hash_value
-        }
-    
+
+        return {"table_ref": str(table_path), "hash": hash_value}
+
     def write_fig(self, name: str, fig_ref: str) -> Dict[str, Any]:
         """Copy figure to figures directory."""
         import shutil
-        
+
         source_path = Path(fig_ref)
         if not source_path.exists():
             raise FileNotFoundError(f"Figure not found: {fig_ref}")
-        
+
         dest_path = self.figures_dir / f"{name}{source_path.suffix}"
         shutil.copy2(source_path, dest_path)
-        
-        return {
-            "fig_ref": str(dest_path)
-        }
-    
+
+        return {"fig_ref": str(dest_path)}
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -335,74 +329,69 @@
                         "type": "object",
                         "properties": {
                             "kind": {
                                 "type": "string",
                                 "enum": ["one_pager", "appendix"],
-                                "description": "Type of report to generate"
+                                "description": "Type of report to generate",
                             },
                             "payload": {
                                 "type": "object",
-                                "description": "Report content payload"
+                                "description": "Report content payload",
                             },
                             "format": {
                                 "type": "string",
                                 "enum": ["html", "md"],
-                                "description": "Report format (default: html)"
-                            }
+                                "description": "Report format (default: html)",
+                            },
                         },
-                        "required": ["kind", "payload"]
-                    }
-                }
+                        "required": ["kind", "payload"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "ArtifactStore_write_table",
                     "description": "Write table object to JSON file",
                     "parameters": {
                         "type": "object",
                         "properties": {
-                            "name": {
-                                "type": "string",
-                                "description": "Table name"
-                            },
+                            "name": {"type": "string", "description": "Table name"},
                             "obj": {
                                 "type": "object",
-                                "description": "Table object to save"
-                            }
+                                "description": "Table object to save",
+                            },
                         },
-                        "required": ["name", "obj"]
-                    }
-                }
-            }
+                        "required": ["name", "obj"],
+                    },
+                },
+            },
         ]
 
 
 class HandoffLedger:
     """Wrapper for handoff ledger functionality."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.ledger = BaseHandoffLedger(artifacts_dir / "logs" / "handoff_ledger.jsonl")
-    
+
     def append(
         self,
         job_id: str,
         stage: str,
         input_refs: List[str],
         output_refs: List[str],
         schema_uri: str,
-        hash_value: str
+        hash_value: str,
     ) -> Dict[str, Any]:
         """Append handoff entry."""
-        self.ledger.append(job_id, stage, input_refs, output_refs, schema_uri, hash_value)
-        
-        return {
-            "status": "logged",
-            "stage": stage,
-            "outputs": len(output_refs)
-        }
-    
+        self.ledger.append(
+            job_id, stage, input_refs, output_refs, schema_uri, hash_value
+        )
+
+        return {"status": "logged", "stage": stage, "outputs": len(output_refs)}
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -410,37 +399,41 @@
                     "name": "HandoffLedger_append",
                     "description": "Log a handoff between pipeline stages",
                     "parameters": {
                         "type": "object",
                         "properties": {
-                            "job_id": {
-                                "type": "string",
-                                "description": "Job ID"
-                            },
+                            "job_id": {"type": "string", "description": "Job ID"},
                             "stage": {
                                 "type": "string",
-                                "description": "Pipeline stage name"
+                                "description": "Pipeline stage name",
                             },
                             "input_refs": {
                                 "type": "array",
                                 "items": {"type": "string"},
-                                "description": "Input file references"
+                                "description": "Input file references",
                             },
                             "output_refs": {
                                 "type": "array",
                                 "items": {"type": "string"},
-                                "description": "Output file references"
+                                "description": "Output file references",
                             },
                             "schema_uri": {
                                 "type": "string",
-                                "description": "Schema URI for validation"
+                                "description": "Schema URI for validation",
                             },
                             "hash_value": {
                                 "type": "string",
-                                "description": "Hash of the handoff data"
-                            }
+                                "description": "Hash of the handoff data",
+                            },
                         },
-                        "required": ["job_id", "stage", "input_refs", "output_refs", "schema_uri", "hash_value"]
-                    }
-                }
+                        "required": [
+                            "job_id",
+                            "stage",
+                            "input_refs",
+                            "output_refs",
+                            "schema_uri",
+                            "hash_value",
+                        ],
+                    },
+                },
             }
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/artifacts.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/metrics.py	2025-09-04 23:56:24.376974+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/metrics.py	2025-10-17 00:12:38.174118+00:00
@@ -3,213 +3,238 @@
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
 import numpy as np
 from sklearn.metrics import (
-    accuracy_score, f1_score, precision_score, recall_score,
-    roc_auc_score, average_precision_score,
-    mean_absolute_error, mean_squared_error, r2_score
+    accuracy_score,
+    f1_score,
+    precision_score,
+    recall_score,
+    roc_auc_score,
+    average_precision_score,
+    mean_absolute_error,
+    mean_squared_error,
+    r2_score,
 )
 
 from ..utils import load_pickle
 
 
 class Metrics:
     """Metrics calculation and statistical analysis."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
-    
+
     def evaluate(
         self,
         model_ref: str,
         X_test_ref: str,
         y_test_ref: str,
-        task_type: Optional[str] = None
+        task_type: Optional[str] = None,
     ) -> Dict[str, Any]:
         """
         Evaluate model on test set with comprehensive metrics.
-        
+
         Args:
             model_ref: Reference to trained model
             X_test_ref: Test features reference
             y_test_ref: Test target reference
             task_type: 'classification' or 'regression' (auto-inferred if None)
-            
+
         Returns:
             Comprehensive evaluation metrics
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_test = load_pickle(X_test_ref)
         y_test = load_pickle(y_test_ref)
-        
+
         # Infer task type if not provided
         if task_type is None:
-            task_type = "classification" if len(np.unique(y_test)) <= 20 else "regression"
-        
+            task_type = (
+                "classification" if len(np.unique(y_test)) <= 20 else "regression"
+            )
+
         # Get predictions
         y_pred = model.predict(X_test)
-        
+
         metrics = {"task_type": task_type}
-        
+
         if task_type == "classification":
             # Classification metrics
-            metrics.update({
-                "accuracy": float(accuracy_score(y_test, y_pred)),
-                "f1_score": float(f1_score(y_test, y_pred, average='weighted')),
-                "precision": float(precision_score(y_test, y_pred, average='weighted')),
-                "recall": float(recall_score(y_test, y_pred, average='weighted'))
-            })
-            
+            metrics.update(
+                {
+                    "accuracy": float(accuracy_score(y_test, y_pred)),
+                    "f1_score": float(f1_score(y_test, y_pred, average="weighted")),
+                    "precision": float(
+                        precision_score(y_test, y_pred, average="weighted")
+                    ),
+                    "recall": float(recall_score(y_test, y_pred, average="weighted")),
+                }
+            )
+
             # ROC AUC and PR AUC for binary classification
             if len(np.unique(y_test)) == 2:
                 try:
                     if hasattr(model, "predict_proba"):
                         y_proba = model.predict_proba(X_test)[:, 1]
                         metrics["roc_auc"] = float(roc_auc_score(y_test, y_proba))
-                        metrics["pr_auc"] = float(average_precision_score(y_test, y_proba))
+                        metrics["pr_auc"] = float(
+                            average_precision_score(y_test, y_proba)
+                        )
                 except Exception:
                     pass
-        
+
         else:
             # Regression metrics
-            metrics.update({
-                "mae": float(mean_absolute_error(y_test, y_pred)),
-                "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
-                "r2": float(r2_score(y_test, y_pred))
-            })
-            
+            metrics.update(
+                {
+                    "mae": float(mean_absolute_error(y_test, y_pred)),
+                    "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
+                    "r2": float(r2_score(y_test, y_pred)),
+                }
+            )
+
             # Additional regression metrics
-            mape = np.mean(np.abs((y_test - y_pred) / np.maximum(np.abs(y_test), 1e-8))) * 100
+            mape = (
+                np.mean(np.abs((y_test - y_pred) / np.maximum(np.abs(y_test), 1e-8)))
+                * 100
+            )
             metrics["mape"] = float(mape)
-        
+
         return metrics
-    
+
     def bootstrap_ci(
         self,
         y_true: np.ndarray,
         y_pred: np.ndarray,
         metric_name: str,
         n_bootstrap: int = 1000,
-        confidence: float = 0.95
+        confidence: float = 0.95,
     ) -> Dict[str, float]:
         """
         Calculate bootstrap confidence intervals for a metric.
-        
+
         Args:
             y_true: True values
             y_pred: Predicted values
             metric_name: Name of metric to calculate
             n_bootstrap: Number of bootstrap samples
             confidence: Confidence level
-            
+
         Returns:
             Mean, lower, and upper bounds
         """
         metric_func = self._get_metric_function(metric_name)
-        
+
         # Bootstrap sampling
         n_samples = len(y_true)
         bootstrap_scores = []
-        
+
         np.random.seed(42)  # For reproducibility
         for _ in range(n_bootstrap):
             # Sample with replacement
             indices = np.random.choice(n_samples, size=n_samples, replace=True)
             y_true_boot = y_true[indices]
             y_pred_boot = y_pred[indices]
-            
+
             try:
                 score = metric_func(y_true_boot, y_pred_boot)
                 bootstrap_scores.append(score)
             except Exception:
                 continue
-        
+
         bootstrap_scores = np.array(bootstrap_scores)
-        
+
         # Calculate confidence interval
         alpha = 1 - confidence
         lower_percentile = (alpha / 2) * 100
         upper_percentile = (1 - alpha / 2) * 100
-        
+
         return {
             "mean": float(np.mean(bootstrap_scores)),
             "lower": float(np.percentile(bootstrap_scores, lower_percentile)),
             "upper": float(np.percentile(bootstrap_scores, upper_percentile)),
             "std": float(np.std(bootstrap_scores)),
-            "n_bootstrap": len(bootstrap_scores)
+            "n_bootstrap": len(bootstrap_scores),
         }
-    
+
     def _get_metric_function(self, metric_name: str):
         """Get metric function by name."""
         metric_functions = {
             "accuracy": accuracy_score,
-            "f1_score": lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted'),
-            "precision": lambda y_true, y_pred: precision_score(y_true, y_pred, average='weighted'),
-            "recall": lambda y_true, y_pred: recall_score(y_true, y_pred, average='weighted'),
+            "f1_score": lambda y_true, y_pred: f1_score(
+                y_true, y_pred, average="weighted"
+            ),
+            "precision": lambda y_true, y_pred: precision_score(
+                y_true, y_pred, average="weighted"
+            ),
+            "recall": lambda y_true, y_pred: recall_score(
+                y_true, y_pred, average="weighted"
+            ),
             "mae": mean_absolute_error,
             "rmse": lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),
-            "r2": r2_score
+            "r2": r2_score,
         }
-        
+
         if metric_name not in metric_functions:
             raise ValueError(f"Unknown metric: {metric_name}")
-        
+
         return metric_functions[metric_name]
-    
+
     def compare_models(
-        self,
-        model_results: List[Dict[str, Any]],
-        primary_metric: str
+        self, model_results: List[Dict[str, Any]], primary_metric: str
     ) -> Dict[str, Any]:
         """
         Compare multiple models and rank them.
-        
+
         Args:
             model_results: List of model evaluation results
             primary_metric: Primary metric for ranking
-            
+
         Returns:
             Ranked comparison results
         """
         if not model_results:
             return {"rankings": [], "best_model": None}
-        
+
         # Extract primary metric scores
         scores = []
         for result in model_results:
             metrics = result.get("val_metrics", {})
             score = metrics.get(primary_metric)
             if score is not None:
                 scores.append((score, result))
-        
+
         # Sort by primary metric (higher is better for most metrics except MAE, RMSE)
         reverse_sort = primary_metric not in ["mae", "rmse", "mape"]
         scores.sort(key=lambda x: x[0], reverse=reverse_sort)
-        
+
         # Create rankings
         rankings = []
         for rank, (score, result) in enumerate(scores, 1):
-            rankings.append({
-                "rank": rank,
-                "model_id": result.get("model_id", f"model_{rank}"),
-                "model_spec": result.get("model_spec", {}),
-                "primary_score": score,
-                "all_metrics": result.get("val_metrics", {}),
-                "train_time": result.get("train_time_seconds", 0)
-            })
-        
+            rankings.append(
+                {
+                    "rank": rank,
+                    "model_id": result.get("model_id", f"model_{rank}"),
+                    "model_spec": result.get("model_spec", {}),
+                    "primary_score": score,
+                    "all_metrics": result.get("val_metrics", {}),
+                    "train_time": result.get("train_time_seconds", 0),
+                }
+            )
+
         best_model = rankings[0] if rankings else None
-        
+
         return {
             "rankings": rankings,
             "best_model": best_model,
             "primary_metric": primary_metric,
-            "total_models": len(rankings)
+            "total_models": len(rankings),
         }
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -219,29 +244,29 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test features"
+                                "description": "Reference to test features",
                             },
                             "y_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test target"
+                                "description": "Reference to test target",
                             },
                             "task_type": {
                                 "type": "string",
                                 "enum": ["classification", "regression"],
-                                "description": "Task type (auto-inferred if not provided)"
-                            }
+                                "description": "Task type (auto-inferred if not provided)",
+                            },
                         },
-                        "required": ["model_ref", "X_test_ref", "y_test_ref"]
-                    }
-                }
+                        "required": ["model_ref", "X_test_ref", "y_test_ref"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "Metrics_bootstrap_ci",
@@ -249,34 +274,47 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test features"
+                                "description": "Reference to test features",
                             },
                             "y_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test target"
+                                "description": "Reference to test target",
                             },
                             "metric_name": {
                                 "type": "string",
-                                "enum": ["accuracy", "f1_score", "precision", "recall", "mae", "rmse", "r2"],
-                                "description": "Metric to calculate CI for"
+                                "enum": [
+                                    "accuracy",
+                                    "f1_score",
+                                    "precision",
+                                    "recall",
+                                    "mae",
+                                    "rmse",
+                                    "r2",
+                                ],
+                                "description": "Metric to calculate CI for",
                             },
                             "n_bootstrap": {
                                 "type": "integer",
-                                "description": "Number of bootstrap samples (default: 1000)"
+                                "description": "Number of bootstrap samples (default: 1000)",
                             },
                             "confidence": {
                                 "type": "number",
-                                "description": "Confidence level (default: 0.95)"
-                            }
+                                "description": "Confidence level (default: 0.95)",
+                            },
                         },
-                        "required": ["model_ref", "X_test_ref", "y_test_ref", "metric_name"]
-                    }
-                }
-            }
+                        "required": [
+                            "model_ref",
+                            "X_test_ref",
+                            "y_test_ref",
+                            "metric_name",
+                        ],
+                    },
+                },
+            },
         ]
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/pii.py	2025-09-05 00:00:25.452393+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/pii.py	2025-10-17 00:12:38.173242+00:00
@@ -9,229 +9,277 @@
 from ..utils import load_pickle, save_pickle
 
 
 class PII:
     """PII detection and handling tools."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.data_dir = artifacts_dir / "data"
         self.data_dir.mkdir(parents=True, exist_ok=True)
-        
+
         # PII patterns
         self.patterns = {
-            "email": r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
-            "phone": r'(\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})',
-            "ssn": r'\b\d{3}-?\d{2}-?\d{4}\b',
-            "credit_card": r'\b(?:\d{4}[-\s]?){3}\d{4}\b',
-            "ip_address": r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b',
-            "url": r'https?://(?:[-\w.])+(?:[:\d]+)?(?:/(?:[\w/_.])*(?:\?(?:[\w&=%.])*)?(?:#(?:\w*))?)?'
+            "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
+            "phone": r"(\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})",
+            "ssn": r"\b\d{3}-?\d{2}-?\d{4}\b",
+            "credit_card": r"\b(?:\d{4}[-\s]?){3}\d{4}\b",
+            "ip_address": r"\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b",
+            "url": r"https?://(?:[-\w.])+(?:[:\d]+)?(?:/(?:[\w/_.])*(?:\?(?:[\w&=%.])*)?(?:#(?:\w*))?)?",
         }
-    
-    def scan(
-        self,
-        df_ref: str,
-        patterns: Optional[List[str]] = None
-    ) -> Dict[str, Any]:
+
+    def scan(self, df_ref: str, patterns: Optional[List[str]] = None) -> Dict[str, Any]:
         """
         Scan DataFrame for PII patterns.
-        
+
         Args:
             df_ref: Reference to DataFrame
             patterns: List of pattern names to check (default: all)
-            
+
         Returns:
             PII scan results
         """
         df = load_pickle(df_ref)
         patterns = patterns or list(self.patterns.keys())
-        
+
         findings = {}
         total_matches = 0
-        
+
         for pattern_name in patterns:
             if pattern_name not in self.patterns:
                 continue
-            
+
             pattern = self.patterns[pattern_name]
             pattern_findings = {}
-            
+
             # Check each column
             for col in df.columns:
-                if df[col].dtype == 'object':  # Only check string columns
+                if df[col].dtype == "object":  # Only check string columns
                     matches = []
-                    
+
                     for idx, value in df[col].items():
                         if pd.isna(value):
                             continue
-                        
+
                         value_str = str(value)
                         found_matches = re.findall(pattern, value_str, re.IGNORECASE)
-                        
+
                         if found_matches:
-                            matches.extend([
-                                {
-                                    "row": int(idx),
-                                    "value": match if isinstance(match, str) else str(match),
-                                    "context": value_str[:50] + "..." if len(value_str) > 50 else value_str
-                                }
-                                for match in found_matches
-                            ])
-                    
+                            matches.extend(
+                                [
+                                    {
+                                        "row": int(idx),
+                                        "value": (
+                                            match
+                                            if isinstance(match, str)
+                                            else str(match)
+                                        ),
+                                        "context": (
+                                            value_str[:50] + "..."
+                                            if len(value_str) > 50
+                                            else value_str
+                                        ),
+                                    }
+                                    for match in found_matches
+                                ]
+                            )
+
                     if matches:
                         pattern_findings[col] = {
                             "count": len(matches),
                             "samples": matches[:5],  # First 5 matches
-                            "total_rows_affected": len(set(m["row"] for m in matches))
+                            "total_rows_affected": len(set(m["row"] for m in matches)),
                         }
                         total_matches += len(matches)
-            
+
             if pattern_findings:
                 findings[pattern_name] = pattern_findings
-        
+
         # Determine risk level
         if total_matches == 0:
             risk_level = "none"
         elif total_matches < 10:
             risk_level = "low"
         elif total_matches < 100:
             risk_level = "medium"
         else:
             risk_level = "high"
-        
+
         return {
             "risk_level": risk_level,
             "total_matches": total_matches,
             "patterns_found": list(findings.keys()),
             "findings": findings,
             "summary": {
-                "columns_affected": len(set(
-                    col for pattern_findings in findings.values() 
-                    for col in pattern_findings.keys()
-                )),
-                "rows_affected": len(set(
-                    match["row"] for pattern_findings in findings.values()
-                    for col_findings in pattern_findings.values()
-                    for match in col_findings["samples"]
-                ))
-            }
+                "columns_affected": len(
+                    set(
+                        col
+                        for pattern_findings in findings.values()
+                        for col in pattern_findings.keys()
+                    )
+                ),
+                "rows_affected": len(
+                    set(
+                        match["row"]
+                        for pattern_findings in findings.values()
+                        for col_findings in pattern_findings.values()
+                        for match in col_findings["samples"]
+                    )
+                ),
+            },
         }
-    
+
     def redact(
         self,
         df_ref: str,
         patterns: Optional[List[str]] = None,
-        replacement: str = "[REDACTED]"
+        replacement: str = "[REDACTED]",
     ) -> Dict[str, Any]:
         """
         Redact PII from DataFrame.
-        
+
         Args:
             df_ref: Reference to DataFrame
             patterns: List of pattern names to redact (default: all)
             replacement: Replacement string for PII
-            
+
         Returns:
             Reference to redacted DataFrame and redaction summary
         """
         df = load_pickle(df_ref)
         patterns = patterns or list(self.patterns.keys())
-        
+
         df_redacted = df.copy()
         redaction_log = {}
         total_redactions = 0
-        
+
         for pattern_name in patterns:
             if pattern_name not in self.patterns:
                 continue
-            
+
             pattern = self.patterns[pattern_name]
             pattern_redactions = {}
-            
+
             # Redact each column
             for col in df.columns:
-                if df[col].dtype == 'object':  # Only process string columns
+                if df[col].dtype == "object":  # Only process string columns
                     redactions_in_col = 0
-                    
+
                     def redact_match(match):
                         nonlocal redactions_in_col
                         redactions_in_col += 1
                         return replacement
-                    
+
                     # Apply redaction
-                    df_redacted[col] = df_redacted[col].astype(str).str.replace(
-                        pattern, redact_match, regex=True, case=False
-                    )
-                    
+                    df_redacted[col] = (
+                        df_redacted[col]
+                        .astype(str)
+                        .str.replace(pattern, redact_match, regex=True, case=False)
+                    )
+
                     if redactions_in_col > 0:
                         pattern_redactions[col] = redactions_in_col
                         total_redactions += redactions_in_col
-            
+
             if pattern_redactions:
                 redaction_log[pattern_name] = pattern_redactions
-        
+
         # Save redacted DataFrame
         from ..utils import hash_dataset
+
         dataset_hash = hash_dataset(df_redacted, "redacted")
         redacted_path = self.data_dir / f"redacted_{dataset_hash[:8]}.pkl"
         save_pickle(df_redacted, redacted_path)
-        
+
         return {
             "df_ref_sanitized": str(redacted_path),
             "total_redactions": total_redactions,
             "redaction_log": redaction_log,
             "summary": {
                 "patterns_redacted": list(redaction_log.keys()),
-                "columns_affected": len(set(
-                    col for pattern_redactions in redaction_log.values()
-                    for col in pattern_redactions.keys()
-                )),
-                "replacement_string": replacement
-            }
+                "columns_affected": len(
+                    set(
+                        col
+                        for pattern_redactions in redaction_log.values()
+                        for col in pattern_redactions.keys()
+                    )
+                ),
+                "replacement_string": replacement,
+            },
         }
-    
+
     def check_column_names(self, df_ref: str) -> Dict[str, Any]:
         """
         Check column names for potential PII indicators.
-        
+
         Args:
             df_ref: Reference to DataFrame
-            
+
         Returns:
             Column name analysis
         """
         df = load_pickle(df_ref)
-        
+
         suspicious_keywords = [
-            'email', 'mail', 'phone', 'tel', 'ssn', 'social', 'security',
-            'credit', 'card', 'account', 'password', 'pass', 'secret',
-            'name', 'first', 'last', 'fname', 'lname', 'address', 'addr',
-            'street', 'city', 'zip', 'postal', 'dob', 'birth', 'age'
+            "email",
+            "mail",
+            "phone",
+            "tel",
+            "ssn",
+            "social",
+            "security",
+            "credit",
+            "card",
+            "account",
+            "password",
+            "pass",
+            "secret",
+            "name",
+            "first",
+            "last",
+            "fname",
+            "lname",
+            "address",
+            "addr",
+            "street",
+            "city",
+            "zip",
+            "postal",
+            "dob",
+            "birth",
+            "age",
         ]
-        
+
         suspicious_columns = []
-        
+
         for col in df.columns:
             col_lower = col.lower()
             for keyword in suspicious_keywords:
                 if keyword in col_lower:
-                    suspicious_columns.append({
-                        "column": col,
-                        "keyword": keyword,
-                        "risk_level": "high" if keyword in ['ssn', 'social', 'password', 'secret'] else "medium"
-                    })
+                    suspicious_columns.append(
+                        {
+                            "column": col,
+                            "keyword": keyword,
+                            "risk_level": (
+                                "high"
+                                if keyword in ["ssn", "social", "password", "secret"]
+                                else "medium"
+                            ),
+                        }
+                    )
                     break
-        
+
         return {
             "suspicious_columns": suspicious_columns,
             "total_suspicious": len(suspicious_columns),
             "high_risk_columns": [
-                col["column"] for col in suspicious_columns 
+                col["column"]
+                for col in suspicious_columns
                 if col["risk_level"] == "high"
-            ]
+            ],
         }
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -241,24 +289,31 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference to pickled DataFrame"
+                                "description": "Reference to pickled DataFrame",
                             },
                             "patterns": {
                                 "type": "array",
                                 "items": {
                                     "type": "string",
-                                    "enum": ["email", "phone", "ssn", "credit_card", "ip_address", "url"]
+                                    "enum": [
+                                        "email",
+                                        "phone",
+                                        "ssn",
+                                        "credit_card",
+                                        "ip_address",
+                                        "url",
+                                    ],
                                 },
-                                "description": "List of PII patterns to check (default: all)"
-                            }
+                                "description": "List of PII patterns to check (default: all)",
+                            },
                         },
-                        "required": ["df_ref"]
-                    }
-                }
+                        "required": ["df_ref"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "PII_redact",
@@ -266,28 +321,35 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference to pickled DataFrame"
+                                "description": "Reference to pickled DataFrame",
                             },
                             "patterns": {
                                 "type": "array",
                                 "items": {
                                     "type": "string",
-                                    "enum": ["email", "phone", "ssn", "credit_card", "ip_address", "url"]
+                                    "enum": [
+                                        "email",
+                                        "phone",
+                                        "ssn",
+                                        "credit_card",
+                                        "ip_address",
+                                        "url",
+                                    ],
                                 },
-                                "description": "List of PII patterns to redact (default: all)"
+                                "description": "List of PII patterns to redact (default: all)",
                             },
                             "replacement": {
                                 "type": "string",
-                                "description": "Replacement string for PII (default: [REDACTED])"
-                            }
+                                "description": "Replacement string for PII (default: [REDACTED])",
+                            },
                         },
-                        "required": ["df_ref"]
-                    }
-                }
+                        "required": ["df_ref"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "PII_check_column_names",
@@ -295,13 +357,13 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference to pickled DataFrame"
+                                "description": "Reference to pickled DataFrame",
                             }
                         },
-                        "required": ["df_ref"]
-                    }
-                }
-            }
+                        "required": ["df_ref"],
+                    },
+                },
+            },
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/pii.py
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/metrics.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/features.py	2025-09-04 23:55:15.012425+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/features.py	2025-10-17 00:12:38.176259+00:00
@@ -11,118 +11,124 @@
 from ..utils import load_pickle, save_json, save_pickle
 
 
 class Splitter:
     """Data splitting utilities."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.handoffs_dir = artifacts_dir / "handoffs"
         self.handoffs_dir.mkdir(parents=True, exist_ok=True)
-    
+
     def make_splits(
         self,
         df_ref: str,
         target: str,
         policy: str,
         test_size: float = 0.2,
         val_size: float = 0.2,
-        seed: int = 42
+        seed: int = 42,
     ) -> Dict[str, Any]:
         """
         Create train/validation/test splits.
-        
+
         Args:
             df_ref: Reference to DataFrame
             target: Target column name
             policy: 'iid' or 'time'
             test_size: Test set proportion
             val_size: Validation set proportion
             seed: Random seed
-            
+
         Returns:
             Split indices saved to split_indices.json
         """
         df = load_pickle(df_ref)
-        
+
         if target not in df.columns:
             raise ValueError(f"Target column '{target}' not found")
-        
+
         n_samples = len(df)
         indices = np.arange(n_samples)
-        
+
         if policy == "iid":
             # Random stratified splits
             y = df[target]
-            
+
             # First split: train+val vs test
             train_val_idx, test_idx = train_test_split(
-                indices, 
+                indices,
                 test_size=test_size,
                 stratify=y if y.nunique() <= 20 else None,  # Stratify if classification
-                random_state=seed
+                random_state=seed,
             )
-            
+
             # Second split: train vs val
             y_train_val = y.iloc[train_val_idx]
             train_idx, val_idx = train_test_split(
                 train_val_idx,
                 test_size=val_size / (1 - test_size),  # Adjust for remaining data
                 stratify=y_train_val if y_train_val.nunique() <= 20 else None,
-                random_state=seed
+                random_state=seed,
             )
-            
+
         elif policy == "time":
             # Temporal splits (assumes data is sorted by time)
             test_start = int(n_samples * (1 - test_size))
             val_start = int(test_start * (1 - val_size))
-            
+
             train_idx = indices[:val_start]
             val_idx = indices[val_start:test_start]
             test_idx = indices[test_start:]
-            
+
         else:
             raise ValueError(f"Unknown split policy: {policy}")
-        
+
         # Create split info
         split_info = {
             "policy": policy,
             "seed": seed,
             "sizes": {
                 "train": len(train_idx),
-                "val": len(val_idx), 
-                "test": len(test_idx)
+                "val": len(val_idx),
+                "test": len(test_idx),
             },
             "proportions": {
                 "train": len(train_idx) / n_samples,
                 "val": len(val_idx) / n_samples,
-                "test": len(test_idx) / n_samples
+                "test": len(test_idx) / n_samples,
             },
             "indices": {
                 "train": train_idx.tolist(),
                 "val": val_idx.tolist(),
-                "test": test_idx.tolist()
-            }
+                "test": test_idx.tolist(),
+            },
         }
-        
+
         # Add target distribution info
         y = df[target]
         if y.nunique() <= 20:  # Classification
-            for split_name, idx in [("train", train_idx), ("val", val_idx), ("test", test_idx)]:
+            for split_name, idx in [
+                ("train", train_idx),
+                ("val", val_idx),
+                ("test", test_idx),
+            ]:
                 y_split = y.iloc[idx]
-                split_info[f"{split_name}_target_dist"] = y_split.value_counts().to_dict()
-        
+                split_info[f"{split_name}_target_dist"] = (
+                    y_split.value_counts().to_dict()
+                )
+
         # Save splits
         splits_path = self.handoffs_dir / "split_indices.json"
         hash_value = save_json(split_info, splits_path)
-        
+
         return {
             "splits_ref": str(splits_path),
             "hash": hash_value,
-            "summary": split_info["sizes"]
+            "summary": split_info["sizes"],
         }
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -132,216 +138,218 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference to pickled DataFrame"
+                                "description": "Reference to pickled DataFrame",
                             },
                             "target": {
                                 "type": "string",
-                                "description": "Target column name"
+                                "description": "Target column name",
                             },
                             "policy": {
                                 "type": "string",
                                 "enum": ["iid", "time"],
-                                "description": "Splitting policy"
+                                "description": "Splitting policy",
                             },
                             "test_size": {
                                 "type": "number",
-                                "description": "Test set proportion (default: 0.2)"
+                                "description": "Test set proportion (default: 0.2)",
                             },
                             "val_size": {
-                                "type": "number", 
-                                "description": "Validation set proportion (default: 0.2)"
+                                "type": "number",
+                                "description": "Validation set proportion (default: 0.2)",
                             },
                             "seed": {
                                 "type": "integer",
-                                "description": "Random seed (default: 42)"
-                            }
+                                "description": "Random seed (default: 42)",
+                            },
                         },
-                        "required": ["df_ref", "target", "policy"]
-                    }
-                }
+                        "required": ["df_ref", "target", "policy"],
+                    },
+                },
             }
         ]
 
 
 class Featurizer:
     """Feature engineering and preprocessing."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.handoffs_dir = artifacts_dir / "handoffs"
         self.data_dir = artifacts_dir / "data"
         self.handoffs_dir.mkdir(parents=True, exist_ok=True)
         self.data_dir.mkdir(parents=True, exist_ok=True)
-    
+
     def plan(
-        self,
-        df_ref: str,
-        target: str,
-        deny_list: Optional[List[str]] = None
+        self, df_ref: str, target: str, deny_list: Optional[List[str]] = None
     ) -> Dict[str, Any]:
         """
         Create feature engineering plan.
-        
+
         Args:
             df_ref: Reference to DataFrame
             target: Target column name
             deny_list: Columns to exclude
-            
+
         Returns:
             Feature plan saved to feature_plan.json
         """
         df = load_pickle(df_ref)
         deny_list = deny_list or []
-        
+
         # Exclude target and denied columns
-        feature_cols = [col for col in df.columns if col != target and col not in deny_list]
-        
+        feature_cols = [
+            col for col in df.columns if col != target and col not in deny_list
+        ]
+
         plan = {
             "target": target,
             "feature_columns": feature_cols,
             "denied_columns": deny_list,
-            "transforms": []
+            "transforms": [],
         }
-        
+
         # Analyze each feature column
         for col in feature_cols:
             col_info = {
                 "column": col,
                 "dtype": str(df[col].dtype),
                 "missing_count": int(df[col].isnull().sum()),
-                "unique_values": int(df[col].nunique())
+                "unique_values": int(df[col].nunique()),
             }
-            
+
             # Determine transforms needed
             transforms = []
-            
+
             if df[col].isnull().sum() > 0:
-                if df[col].dtype in ['int64', 'float64']:
+                if df[col].dtype in ["int64", "float64"]:
                     transforms.append("impute_median")
                 else:
                     transforms.append("impute_mode")
-            
-            if df[col].dtype in ['object', 'category']:
+
+            if df[col].dtype in ["object", "category"]:
                 if df[col].nunique() > 50:
                     transforms.append("high_cardinality_encode")
                 else:
                     transforms.append("label_encode")
-            elif df[col].dtype in ['int64', 'float64']:
+            elif df[col].dtype in ["int64", "float64"]:
                 transforms.append("standard_scale")
-            
+
             col_info["transforms"] = transforms
             plan["transforms"].append(col_info)
-        
+
         # Add rationale
         plan["rationale"] = {
             "total_features": len(feature_cols),
             "excluded_features": len(deny_list),
-            "numeric_features": len([c for c in feature_cols if df[c].dtype in ['int64', 'float64']]),
-            "categorical_features": len([c for c in feature_cols if df[c].dtype in ['object', 'category']]),
-            "missing_data_columns": len([c for c in feature_cols if df[c].isnull().sum() > 0])
+            "numeric_features": len(
+                [c for c in feature_cols if df[c].dtype in ["int64", "float64"]]
+            ),
+            "categorical_features": len(
+                [c for c in feature_cols if df[c].dtype in ["object", "category"]]
+            ),
+            "missing_data_columns": len(
+                [c for c in feature_cols if df[c].isnull().sum() > 0]
+            ),
         }
-        
+
         # Save plan
         plan_path = self.handoffs_dir / "feature_plan.json"
         hash_value = save_json(plan, plan_path)
-        
+
         return {
             "plan_ref": str(plan_path),
             "hash": hash_value,
-            "summary": plan["rationale"]
+            "summary": plan["rationale"],
         }
-    
-    def apply(
-        self,
-        df_ref: str,
-        plan_ref: str,
-        splits_ref: str
-    ) -> Dict[str, Any]:
+
+    def apply(self, df_ref: str, plan_ref: str, splits_ref: str) -> Dict[str, Any]:
         """
         Apply feature engineering plan to create train/val/test matrices.
-        
+
         Args:
             df_ref: Reference to DataFrame
             plan_ref: Reference to feature plan
             splits_ref: Reference to split indices
-            
+
         Returns:
             References to processed X,y matrices
         """
         from ..utils import load_json
-        
+
         df = load_pickle(df_ref)
         plan = load_json(plan_ref)
         splits = load_json(splits_ref)
-        
+
         target = plan["target"]
         feature_cols = plan["feature_columns"]
-        
+
         # Get splits
         train_idx = splits["indices"]["train"]
         val_idx = splits["indices"]["val"]
         test_idx = splits["indices"]["test"]
-        
+
         # Prepare features and target
         X = df[feature_cols].copy()
         y = df[target].copy()
-        
+
         # Apply transforms (fit on train, transform all)
         X_processed = X.copy()
-        
+
         # Simple preprocessing for MVP
         for col in feature_cols:
-            if X[col].dtype in ['object', 'category']:
+            if X[col].dtype in ["object", "category"]:
                 # Label encoding
                 le = LabelEncoder()
-                X_train_col = X.iloc[train_idx][col].fillna('missing')
+                X_train_col = X.iloc[train_idx][col].fillna("missing")
                 le.fit(X_train_col)
-                
-                X_processed[col] = X[col].fillna('missing').apply(
-                    lambda x: le.transform([x])[0] if x in le.classes_ else -1
+
+                X_processed[col] = (
+                    X[col]
+                    .fillna("missing")
+                    .apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)
                 )
-            elif X[col].dtype in ['int64', 'float64']:
+            elif X[col].dtype in ["int64", "float64"]:
                 # Fill missing with median and scale
                 median_val = X.iloc[train_idx][col].median()
                 X_processed[col] = X[col].fillna(median_val)
-                
+
                 # Standard scaling
                 scaler = StandardScaler()
                 X_train_col = X_processed.iloc[train_idx][[col]]
                 scaler.fit(X_train_col)
                 X_processed[col] = scaler.transform(X_processed[[col]]).flatten()
-        
+
         # Create train/val/test sets
         X_train = X_processed.iloc[train_idx]
         X_val = X_processed.iloc[val_idx]
         X_test = X_processed.iloc[test_idx]
-        
+
         y_train = y.iloc[train_idx]
         y_val = y.iloc[val_idx]
         y_test = y.iloc[test_idx]
-        
+
         # Save matrices
         matrices = {}
         for name, (X_split, y_split) in [
             ("train", (X_train, y_train)),
             ("val", (X_val, y_val)),
-            ("test", (X_test, y_test))
+            ("test", (X_test, y_test)),
         ]:
             X_path = self.data_dir / f"X_{name}.pkl"
             y_path = self.data_dir / f"y_{name}.pkl"
-            
+
             save_pickle(X_split, X_path)
             save_pickle(y_split, y_path)
-            
+
             matrices[f"X_{name}_ref"] = str(X_path)
             matrices[f"y_{name}_ref"] = str(y_path)
-        
+
         return matrices
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -351,25 +359,25 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference to pickled DataFrame"
+                                "description": "Reference to pickled DataFrame",
                             },
                             "target": {
                                 "type": "string",
-                                "description": "Target column name"
+                                "description": "Target column name",
                             },
                             "deny_list": {
                                 "type": "array",
                                 "items": {"type": "string"},
-                                "description": "Columns to exclude from features"
-                            }
+                                "description": "Columns to exclude from features",
+                            },
                         },
-                        "required": ["df_ref", "target"]
-                    }
-                }
+                        "required": ["df_ref", "target"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "Featurizer_apply",
@@ -377,21 +385,21 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference to pickled DataFrame"
+                                "description": "Reference to pickled DataFrame",
                             },
                             "plan_ref": {
                                 "type": "string",
-                                "description": "Reference to feature plan JSON"
+                                "description": "Reference to feature plan JSON",
                             },
                             "splits_ref": {
                                 "type": "string",
-                                "description": "Reference to split indices JSON"
-                            }
+                                "description": "Reference to split indices JSON",
+                            },
                         },
-                        "required": ["df_ref", "plan_ref", "splits_ref"]
-                    }
-                }
-            }
+                        "required": ["df_ref", "plan_ref", "splits_ref"],
+                    },
+                },
+            },
         ]
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/modeling.py	2025-09-04 23:55:49.867931+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/modeling.py	2025-10-17 00:12:38.176558+00:00
@@ -5,116 +5,123 @@
 from typing import Any, Dict, List, Optional, Tuple
 
 import numpy as np
 from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
 from sklearn.linear_model import LogisticRegression, LinearRegression
-from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error
+from sklearn.metrics import (
+    accuracy_score,
+    f1_score,
+    mean_absolute_error,
+    mean_squared_error,
+)
 from sklearn.model_selection import GridSearchCV
 
 from ..utils import load_pickle, save_pickle
 
 
 class ModelTrainer:
     """Model training utilities."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.models_dir = artifacts_dir / "models"
         self.models_dir.mkdir(parents=True, exist_ok=True)
-    
+
     def train(
         self,
         model_spec: Dict[str, Any],
         X_train_ref: str,
         y_train_ref: str,
         X_val_ref: str,
-        y_val_ref: str
+        y_val_ref: str,
     ) -> Dict[str, Any]:
         """
         Train a model with given specification.
-        
+
         Args:
             model_spec: Model specification dict
             X_train_ref: Training features reference
             y_train_ref: Training target reference
             X_val_ref: Validation features reference
             y_val_ref: Validation target reference
-            
+
         Returns:
             Training results with validation metrics
         """
         # Load data
         X_train = load_pickle(X_train_ref)
         y_train = load_pickle(y_train_ref)
         X_val = load_pickle(X_val_ref)
         y_val = load_pickle(y_val_ref)
-        
+
         # Create model
         model = self._create_model(model_spec)
-        
+
         # Train model
         start_time = time.time()
         model.fit(X_train, y_train)
         train_time = time.time() - start_time
-        
+
         # Validate
         y_val_pred = model.predict(X_val)
-        
+
         # Calculate metrics
         task_type = self._infer_task_type(y_train)
         val_metrics = self._calculate_metrics(y_val, y_val_pred, task_type)
-        
+
         # Save model
         model_id = f"{model_spec['name']}_{hash(str(model_spec))}"[:16]
         model_path = self.models_dir / f"{model_id}.pkl"
         save_pickle(model, model_path)
-        
+
         return {
             "model_ref": str(model_path),
             "model_id": model_id,
             "model_spec": model_spec,
             "val_metrics": val_metrics,
             "train_time_seconds": train_time,
-            "task_type": task_type
+            "task_type": task_type,
         }
-    
+
     def _create_model(self, spec: Dict[str, Any]) -> Any:
         """Create model from specification."""
         name = spec["name"]
         params = spec.get("params", {})
-        
+
         if name == "logistic_regression":
             return LogisticRegression(**params, random_state=42)
         elif name == "linear_regression":
             return LinearRegression(**params)
         elif name == "random_forest_classifier":
             return RandomForestClassifier(**params, random_state=42)
         elif name == "random_forest_regressor":
             return RandomForestRegressor(**params, random_state=42)
         else:
             raise ValueError(f"Unknown model: {name}")
-    
+
     def _infer_task_type(self, y: np.ndarray) -> str:
         """Infer if task is classification or regression."""
-        if len(np.unique(y)) <= 20 and y.dtype in ['int64', 'object']:
+        if len(np.unique(y)) <= 20 and y.dtype in ["int64", "object"]:
             return "classification"
         else:
             return "regression"
-    
-    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, task_type: str) -> Dict[str, float]:
+
+    def _calculate_metrics(
+        self, y_true: np.ndarray, y_pred: np.ndarray, task_type: str
+    ) -> Dict[str, float]:
         """Calculate appropriate metrics."""
         metrics = {}
-        
+
         if task_type == "classification":
             metrics["accuracy"] = float(accuracy_score(y_true, y_pred))
-            metrics["f1_score"] = float(f1_score(y_true, y_pred, average='weighted'))
+            metrics["f1_score"] = float(f1_score(y_true, y_pred, average="weighted"))
         else:
             metrics["mae"] = float(mean_absolute_error(y_true, y_pred))
             metrics["rmse"] = float(np.sqrt(mean_squared_error(y_true, y_pred)))
-        
+
         return metrics
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -127,148 +134,162 @@
                             "model_spec": {
                                 "type": "object",
                                 "description": "Model specification with name and params",
                                 "properties": {
                                     "name": {"type": "string"},
-                                    "params": {"type": "object"}
+                                    "params": {"type": "object"},
                                 },
-                                "required": ["name"]
+                                "required": ["name"],
                             },
                             "X_train_ref": {
                                 "type": "string",
-                                "description": "Reference to training features"
+                                "description": "Reference to training features",
                             },
                             "y_train_ref": {
-                                "type": "string", 
-                                "description": "Reference to training target"
+                                "type": "string",
+                                "description": "Reference to training target",
                             },
                             "X_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation features"
+                                "description": "Reference to validation features",
                             },
                             "y_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation target"
-                            }
+                                "description": "Reference to validation target",
+                            },
                         },
-                        "required": ["model_spec", "X_train_ref", "y_train_ref", "X_val_ref", "y_val_ref"]
-                    }
-                }
+                        "required": [
+                            "model_spec",
+                            "X_train_ref",
+                            "y_train_ref",
+                            "X_val_ref",
+                            "y_val_ref",
+                        ],
+                    },
+                },
             }
         ]
 
 
 class Tuner:
     """Hyperparameter tuning utilities."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
-    
+
     def quick_search(
         self,
         model_family: str,
         X_train_ref: str,
         y_train_ref: str,
-        budget_minutes: int = 5
+        budget_minutes: int = 5,
     ) -> Dict[str, Any]:
         """
         Quick hyperparameter search within budget.
-        
+
         Args:
             model_family: Model family name
             X_train_ref: Training features reference
             y_train_ref: Training target reference
             budget_minutes: Time budget in minutes
-            
+
         Returns:
             Best model spec and trial results
         """
         X_train = load_pickle(X_train_ref)
         y_train = load_pickle(y_train_ref)
-        
+
         # Define search spaces
         search_spaces = {
             "random_forest": {
                 "n_estimators": [50, 100, 200],
                 "max_depth": [3, 5, 10, None],
-                "min_samples_split": [2, 5, 10]
+                "min_samples_split": [2, 5, 10],
             },
             "logistic_regression": {
                 "C": [0.1, 1.0, 10.0],
                 "penalty": ["l1", "l2"],
-                "solver": ["liblinear", "lbfgs"]
-            }
+                "solver": ["liblinear", "lbfgs"],
+            },
         }
-        
+
         if model_family not in search_spaces:
             raise ValueError(f"Unknown model family: {model_family}")
-        
+
         # Infer task type and create base model
         task_type = "classification" if len(np.unique(y_train)) <= 20 else "regression"
-        
+
         if model_family == "random_forest":
             if task_type == "classification":
                 base_model = RandomForestClassifier(random_state=42)
             else:
                 base_model = RandomForestRegressor(random_state=42)
         elif model_family == "logistic_regression":
             base_model = LogisticRegression(random_state=42)
-        
+
         # Grid search with time budget
         param_grid = search_spaces[model_family]
-        
+
         # Adjust for regression models
         if task_type == "regression" and model_family == "logistic_regression":
             # Use linear regression instead
             from sklearn.linear_model import Ridge
+
             base_model = Ridge(random_state=42)
             param_grid = {"alpha": [0.1, 1.0, 10.0]}
-        
+
         grid_search = GridSearchCV(
             base_model,
             param_grid,
             cv=3,
-            scoring='accuracy' if task_type == "classification" else 'neg_mean_absolute_error',
-            n_jobs=-1
+            scoring=(
+                "accuracy"
+                if task_type == "classification"
+                else "neg_mean_absolute_error"
+            ),
+            n_jobs=-1,
         )
-        
+
         start_time = time.time()
         grid_search.fit(X_train, y_train)
         search_time = time.time() - start_time
-        
+
         # Extract results
         best_params = grid_search.best_params_
         best_score = grid_search.best_score_
-        
+
         trials = []
-        for i, (params, score) in enumerate(zip(grid_search.cv_results_['params'], 
-                                               grid_search.cv_results_['mean_test_score'])):
-            trials.append({
-                "trial_id": i,
-                "params": params,
-                "score": float(score),
-                "rank": int(grid_search.cv_results_['rank_test_score'][i])
-            })
-        
+        for i, (params, score) in enumerate(
+            zip(
+                grid_search.cv_results_["params"],
+                grid_search.cv_results_["mean_test_score"],
+            )
+        ):
+            trials.append(
+                {
+                    "trial_id": i,
+                    "params": params,
+                    "score": float(score),
+                    "rank": int(grid_search.cv_results_["rank_test_score"][i]),
+                }
+            )
+
         # Create best model spec
         model_name = f"{model_family}_{'classifier' if task_type == 'classification' else 'regressor'}"
         if model_family == "logistic_regression" and task_type == "regression":
             model_name = "ridge_regression"
-        
-        best_spec = {
-            "name": model_name,
-            "params": best_params
-        }
-        
+
+        best_spec = {"name": model_name, "params": best_params}
+
         return {
             "best_spec": best_spec,
             "best_score": float(best_score),
             "search_time_seconds": search_time,
             "trials": trials,
-            "total_trials": len(trials)
+            "total_trials": len(trials),
         }
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -279,25 +300,25 @@
                         "type": "object",
                         "properties": {
                             "model_family": {
                                 "type": "string",
                                 "enum": ["random_forest", "logistic_regression"],
-                                "description": "Model family to tune"
+                                "description": "Model family to tune",
                             },
                             "X_train_ref": {
                                 "type": "string",
-                                "description": "Reference to training features"
+                                "description": "Reference to training features",
                             },
                             "y_train_ref": {
                                 "type": "string",
-                                "description": "Reference to training target"
+                                "description": "Reference to training target",
                             },
                             "budget_minutes": {
                                 "type": "integer",
-                                "description": "Time budget in minutes (default: 5)"
-                            }
+                                "description": "Time budget in minutes (default: 5)",
+                            },
                         },
-                        "required": ["model_family", "X_train_ref", "y_train_ref"]
-                    }
-                }
+                        "required": ["model_family", "X_train_ref", "y_train_ref"],
+                    },
+                },
             }
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/features.py
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/modeling.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/cli/run.py	2025-09-05 00:02:06.612674+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/cli/run.py	2025-10-17 00:12:38.182425+00:00
@@ -18,67 +18,61 @@
         epilog="""
 Examples:
   python -m cli.run --config configs/iris.yaml
   python -m cli.run --config configs/breast_cancer.yaml
   python -m cli.run --config configs/diabetes.yaml
-        """
+        """,
     )
-    
+
     parser.add_argument(
-        "--config",
-        type=str,
-        required=True,
-        help="Path to YAML configuration file"
+        "--config", type=str, required=True, help="Path to YAML configuration file"
     )
-    
-    parser.add_argument(
-        "--verbose",
-        action="store_true",
-        help="Enable verbose output"
-    )
-    
+
+    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
+
     args = parser.parse_args()
-    
+
     try:
         # Load configuration
         print(f"Loading configuration from: {args.config}")
         config = Config.from_yaml(args.config)
-        
+
         # Validate environment (OpenAI API key)
         config.validate_environment()
-        
+
         # Create and run pipeline
         pipeline = Pipeline(config)
         result = pipeline.run()
-        
+
         if result["status"] == "success":
             print(f"\n✅ Pipeline completed successfully!")
             print(f"📁 Artifacts written to: {result['artifacts_dir']}")
-            
+
             # Show cache statistics
             cache_stats = result.get("cache_stats", {})
             if cache_stats:
                 hits = sum(1 for hit in cache_stats.values() if hit)
                 total = len(cache_stats)
                 print(f"💾 Cache: {hits}/{total} hits")
-            
+
             return 0
         else:
             print(f"\n❌ Pipeline failed: {result['error']}")
             print(f"📁 Partial artifacts in: {result['artifacts_dir']}")
             return 1
-            
+
     except FileNotFoundError as e:
         print(f"❌ Configuration file not found: {e}")
         return 1
     except ValueError as e:
         print(f"❌ Configuration error: {e}")
         return 1
     except Exception as e:
         print(f"❌ Unexpected error: {e}")
         if args.verbose:
             import traceback
+
             traceback.print_exc()
         return 1
 
 
 if __name__ == "__main__":
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/cli/run.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/pipeline.py	2025-09-05 00:22:52.422179+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/pipeline.py	2025-10-17 00:12:38.194881+00:00
@@ -12,36 +12,36 @@
 from .utils import ensure_dir, generate_job_id, get_timestamp, Timer
 
 
 class Pipeline:
     """Main pipeline orchestrator."""
-    
+
     def __init__(self, config: Config):
         self.config = config
         self.job_id = generate_job_id()
-        
+
         # Setup directories
         self.artifacts_dir = ensure_dir(Path(config.report.out_dir) / self.job_id)
         self.handoffs_dir = ensure_dir(self.artifacts_dir / "handoffs")
         self.logs_dir = ensure_dir(self.artifacts_dir / "logs")
-        
+
         # Initialize components
         self.cache = Cache(config.cache.dir, config.cache.mode)
         self.decision_log = DecisionLog(self.logs_dir / "decision_log.jsonl")
         self.handoff_ledger = HandoffLedger(self.logs_dir / "handoff_ledger.jsonl")
         self.manifest = RunManifest(self.job_id)
-        
+
         # Initialize LLM router
         self.llm_router = LLMRouter(config.llms.model_dump(), self.decision_log)
-        
+
         # Initialize tools
         self._init_tools()
-        
+
         # Pipeline state
         self.current_stage = "initialization"
         self.stage_outputs: Dict[str, Any] = {}
-        
+
     def _init_tools(self) -> None:
         """Initialize all tools."""
         self.tools = {
             "data_store": DataStore(self.artifacts_dir),
             "schema_profiler": SchemaProfiler(self.artifacts_dir),
@@ -55,143 +55,159 @@
             "fairness": Fairness(self.artifacts_dir),
             "robustness": Robustness(self.artifacts_dir),
             "plotter": Plotter(self.artifacts_dir),
             "artifact_store": ArtifactStore(self.artifacts_dir),
             "handoff_ledger": HandoffLedger(self.artifacts_dir),
-            "budget_guard": BudgetGuard(self.artifacts_dir, self.config.budgets.model_dump()),
-            "pii": PII(self.artifacts_dir)
-        }
-    
+            "budget_guard": BudgetGuard(
+                self.artifacts_dir, self.config.budgets.model_dump()
+            ),
+            "pii": PII(self.artifacts_dir),
+        }
+
     def run(self) -> Dict[str, Any]:
         """Run the complete pipeline."""
         try:
             print(f"Starting pipeline run {self.job_id}")
-            
+
             # Stage 1: Intake & Validation
             self._run_stage("intake_validation", self._stage_intake_validation)
-            
+
             # Stage 2: Profiling & Quality
             self._run_stage("profiling_quality", self._stage_profiling_quality)
-            
+
             # Stage 3: EDA & Hypotheses
             self._run_stage("eda_hypotheses", self._stage_eda_hypotheses)
-            
+
             # Stage 4: Feature Plan & Splits
             self._run_stage("feature_splits", self._stage_feature_splits)
-            
+
             # Stage 5: Model Ladder
             self._run_stage("model_ladder", self._stage_model_ladder)
-            
+
             # Stage 6: Evaluation & Stress
             self._run_stage("evaluation_stress", self._stage_evaluation_stress)
-            
+
             # Stage 7: Reporting & Packaging
             self._run_stage("reporting", self._stage_reporting)
-            
+
             # Save final manifest
             self.manifest.save(self.artifacts_dir / "run_manifest.json")
-            
+
             print(f"Pipeline completed successfully!")
             print(f"Artifacts written to: {self.artifacts_dir}")
-            
+
             return {
                 "status": "success",
                 "job_id": self.job_id,
                 "artifacts_dir": str(self.artifacts_dir),
-                "cache_stats": self.cache.get_hit_stats()
+                "cache_stats": self.cache.get_hit_stats(),
             }
-            
+
         except Exception as e:
             print(f"Pipeline failed: {str(e)}")
             return {
                 "status": "failed",
                 "job_id": self.job_id,
                 "error": str(e),
-                "artifacts_dir": str(self.artifacts_dir)
+                "artifacts_dir": str(self.artifacts_dir),
             }
-    
+
     def _run_stage(self, stage_name: str, stage_func) -> None:
         """Run a pipeline stage with budget monitoring."""
         self.current_stage = stage_name
         print(f"\n=== Stage: {stage_name} ===")
-        
+
         with Timer(stage_name) as timer:
             # Check budget before stage
             budget_status = self.tools["budget_guard"].checkpoint(stage_name)
-            
+
             if budget_status["status"] == "abort":
-                raise RuntimeError(f"Budget exceeded at stage {stage_name}: {budget_status['recommendations']}")
-            
+                raise RuntimeError(
+                    f"Budget exceeded at stage {stage_name}: {budget_status['recommendations']}"
+                )
+
             # Run stage
             stage_output = stage_func()
             self.stage_outputs[stage_name] = stage_output
-            
+
             # Update manifest
-            self.manifest.add_budget_usage(stage_name, {
-                "duration_seconds": timer.elapsed,
-                "budget_status": budget_status["status"]
-            })
-        
+            self.manifest.add_budget_usage(
+                stage_name,
+                {
+                    "duration_seconds": timer.elapsed,
+                    "budget_status": budget_status["status"],
+                },
+            )
+
         print(f"Completed {stage_name} in {timer}")
-    
+
     def _stage_intake_validation(self) -> Dict[str, Any]:
         """Stage 1: Intake & Validation."""
         # Load data
         if self.config.data.source == "builtin":
             data_result = self.tools["data_store"].read_builtin(self.config.data.name)
         else:
             data_result = self.tools["data_store"].read_csv(self.config.data.csv_paths)
-        
+
         # Set target if not specified for CSV
         target = self.config.data.target or data_result.get("target", "target")
-        
+
         # PII check if enabled
         if self.config.pii.enabled:
-            pii_scan = self.tools["pii"].scan(data_result["df_ref"], self.config.pii.patterns)
-            if pii_scan["risk_level"] in ["medium", "high"] and self.config.pii.action == "redact":
-                redact_result = self.tools["pii"].redact(data_result["df_ref"], self.config.pii.patterns)
+            pii_scan = self.tools["pii"].scan(
+                data_result["df_ref"], self.config.pii.patterns
+            )
+            if (
+                pii_scan["risk_level"] in ["medium", "high"]
+                and self.config.pii.action == "redact"
+            ):
+                redact_result = self.tools["pii"].redact(
+                    data_result["df_ref"], self.config.pii.patterns
+                )
                 data_result["df_ref"] = redact_result["df_ref_sanitized"]
-        
+
         # OpenAI decision: Target validity & task type
         decision_prompt = f"""
         Analyze this dataset and confirm:
         1. Is '{target}' a valid target column?
         2. What is the task type (classification/regression)?
         3. Are there any immediate concerns?
         
         Dataset info: {data_result}
         """
-        
+
         decision = self.llm_router.openai_decide(
             stage="intake_validation",
             prompt=decision_prompt,
-            context={"data_info": data_result, "target": target}
-        )
-        
+            context={"data_info": data_result, "target": target},
+        )
+
         return {
             "data_ref": data_result["df_ref"],
             "target": target,
             "task_type": data_result.get("task_hint", "auto"),
             "decision": decision,
-            "pii_handled": self.config.pii.enabled
-        }
-    
+            "pii_handled": self.config.pii.enabled,
+        }
+
     def _stage_profiling_quality(self) -> Dict[str, Any]:
         """Stage 2: Profiling & Quality."""
         intake_output = self.stage_outputs["intake_validation"]
-        
+
         # Profile dataset
-        profile_result = self.tools["schema_profiler"].profile(intake_output["data_ref"])
-        
+        profile_result = self.tools["schema_profiler"].profile(
+            intake_output["data_ref"]
+        )
+
         # Quality scan
         leakage_result = self.tools["quality_guard"].leakage_scan(
             intake_output["data_ref"],
             intake_output["target"],
             self.config.split.policy,
-            self.config.split.time_col if self.config.split.policy == "time" else None
-        )
-        
+            self.config.split.time_col if self.config.split.policy == "time" else None,
+        )
+
         # OpenAI decision: Proceed or block based on quality
         decision_prompt = f"""
         Review the data quality scan results:
         
         Profile: {profile_result['summary']}
@@ -200,47 +216,49 @@
         Decision needed:
         1. Should we proceed with this data?
         2. Are there any blocking issues?
         3. What columns should be denied from features?
         """
-        
+
         decision = self.llm_router.openai_decide(
             stage="profiling_quality",
             prompt=decision_prompt,
-            context={
-                "profile": profile_result,
-                "leakage": leakage_result
-            }
-        )
-        
+            context={"profile": profile_result, "leakage": leakage_result},
+        )
+
         if leakage_result["status"] == "block":
-            raise RuntimeError(f"Data quality check failed: {leakage_result['offenders']}")
-        
+            raise RuntimeError(
+                f"Data quality check failed: {leakage_result['offenders']}"
+            )
+
         return {
             "profile_ref": profile_result["profile_ref"],
             "leakage_status": leakage_result["status"],
             "quality_decision": decision,
-            "deny_list": [item["column"] for item in leakage_result.get("offenders", [])]
-        }
-    
+            "deny_list": [
+                item["column"] for item in leakage_result.get("offenders", [])
+            ],
+        }
+
     def _stage_eda_hypotheses(self) -> Dict[str, Any]:
         """Stage 3: EDA & Hypotheses (simplified for MVP)."""
         # For MVP, generate basic insights
         intake_output = self.stage_outputs["intake_validation"]
         profile_output = self.stage_outputs["profiling_quality"]
-        
+
         # Load profile for insights
         from .utils import load_json
+
         profile = load_json(profile_output["profile_ref"])
-        
+
         # Generate basic insights
         insights = [
             f"Dataset has {profile['shape']['rows']} rows and {profile['shape']['columns']} columns",
             f"Missing data: {profile['missing_values']['total_missing']} total missing values",
-            f"Data types: {len(profile['column_types']['numeric'])} numeric, {len(profile['column_types']['categorical'])} categorical"
+            f"Data types: {len(profile['column_types']['numeric'])} numeric, {len(profile['column_types']['categorical'])} categorical",
         ]
-        
+
         # OpenAI decision: Select top insights and hypotheses
         decision_prompt = f"""
         Based on the data profile, select the top 3 insights and generate 5 testable hypotheses.
         
         Available insights: {insights}
@@ -248,52 +266,56 @@
         
         Provide:
         1. Top 3 most important insights
         2. 5 testable hypotheses about the data/target relationship
         """
-        
+
         decision = self.llm_router.openai_decide(
             stage="eda_hypotheses",
             prompt=decision_prompt,
-            context={"profile": profile, "insights": insights}
-        )
-        
+            context={"profile": profile, "insights": insights},
+        )
+
         return {
             "top_insights": insights[:3],  # Simplified for MVP
-            "hypotheses": ["Hypothesis 1", "Hypothesis 2", "Hypothesis 3", "Hypothesis 4", "Hypothesis 5"],
-            "eda_decision": decision
-        }
-    
+            "hypotheses": [
+                "Hypothesis 1",
+                "Hypothesis 2",
+                "Hypothesis 3",
+                "Hypothesis 4",
+                "Hypothesis 5",
+            ],
+            "eda_decision": decision,
+        }
+
     def _stage_feature_splits(self) -> Dict[str, Any]:
         """Stage 4: Feature Plan & Splits."""
         intake_output = self.stage_outputs["intake_validation"]
         quality_output = self.stage_outputs["profiling_quality"]
-        
+
         # Create feature plan
         feature_plan = self.tools["featurizer"].plan(
             intake_output["data_ref"],
             intake_output["target"],
-            quality_output["deny_list"]
-        )
-        
+            quality_output["deny_list"],
+        )
+
         # Create splits
         splits = self.tools["splitter"].make_splits(
             intake_output["data_ref"],
             intake_output["target"],
             self.config.split.policy,
             self.config.split.test_size,
             self.config.split.val_size,
-            self.config.split.seed
-        )
-        
+            self.config.split.seed,
+        )
+
         # Apply feature engineering
         matrices = self.tools["featurizer"].apply(
-            intake_output["data_ref"],
-            feature_plan["plan_ref"],
-            splits["splits_ref"]
-        )
-        
+            intake_output["data_ref"], feature_plan["plan_ref"], splits["splits_ref"]
+        )
+
         # OpenAI decision: Approve feature plan and split policy
         decision_prompt = f"""
         Review the feature engineering plan and data splits:
         
         Feature plan: {feature_plan['summary']}
@@ -302,32 +324,29 @@
         Decision needed:
         1. Approve the feature engineering approach?
         2. Confirm the split policy is appropriate?
         3. Any concerns about the feature set?
         """
-        
+
         decision = self.llm_router.openai_decide(
             stage="feature_splits",
             prompt=decision_prompt,
-            context={
-                "feature_plan": feature_plan,
-                "splits": splits
-            }
-        )
-        
+            context={"feature_plan": feature_plan, "splits": splits},
+        )
+
         return {
             "feature_plan_ref": feature_plan["plan_ref"],
             "splits_ref": splits["splits_ref"],
             "matrices": matrices,
-            "feature_decision": decision
-        }
-    
+            "feature_decision": decision,
+        }
+
     def _stage_model_ladder(self) -> Dict[str, Any]:
         """Stage 5: Model Ladder."""
         feature_output = self.stage_outputs["feature_splits"]
         matrices = feature_output["matrices"]
-        
+
         # Determine task type from intake
         intake_output = self.stage_outputs["intake_validation"]
         task_type = intake_output.get("task_type", "auto")
 
         # Define model candidates based on task type
@@ -339,29 +358,29 @@
         else:  # classification
             model_specs = [
                 {"name": "logistic_regression", "params": {}},
                 {"name": "random_forest_classifier", "params": {"n_estimators": 100}},
             ]
-        
+
         # Train models
         model_results = []
         for spec in model_specs:
             try:
                 result = self.tools["model_trainer"].train(
                     spec,
                     matrices["X_train_ref"],
                     matrices["y_train_ref"],
                     matrices["X_val_ref"],
-                    matrices["y_val_ref"]
+                    matrices["y_val_ref"],
                 )
                 model_results.append(result)
             except Exception as e:
                 print(f"Failed to train {spec['name']}: {e}")
-        
+
         # Compare models
         comparison = self.tools["metrics"].compare_models(model_results, "accuracy")
-        
+
         # OpenAI decision: Select best model
         decision_prompt = f"""
         Review the model comparison results and select the best model:
         
         Model rankings: {comparison['rankings']}
@@ -369,29 +388,29 @@
         Decision needed:
         1. Which model should be selected?
         2. What is the rationale for this choice?
         3. What operating threshold should be used?
         """
-        
+
         decision = self.llm_router.openai_decide(
             stage="model_ladder",
             prompt=decision_prompt,
-            context={"comparison": comparison}
-        )
-        
+            context={"comparison": comparison},
+        )
+
         return {
             "model_results": model_results,
             "comparison": comparison,
             "selected_model": comparison["best_model"],
-            "model_decision": decision
-        }
-    
+            "model_decision": decision,
+        }
+
     def _stage_evaluation_stress(self) -> Dict[str, Any]:
         """Stage 6: Evaluation & Stress Testing."""
         ladder_output = self.stage_outputs["model_ladder"]
         feature_output = self.stage_outputs["feature_splits"]
-        
+
         selected_model = ladder_output.get("selected_model")
         matrices = feature_output["matrices"]
 
         # Find the model reference from the results
         selected_model_ref = None
@@ -405,22 +424,18 @@
             # Fallback to first model if selection failed
             selected_model_ref = ladder_output["model_results"][0]["model_ref"]
 
         # Test evaluation
         test_metrics = self.tools["metrics"].evaluate(
-            selected_model_ref,
-            matrices["X_test_ref"],
-            matrices["y_test_ref"]
-        )
-        
+            selected_model_ref, matrices["X_test_ref"], matrices["y_test_ref"]
+        )
+
         # Robustness testing
         robustness_result = self.tools["robustness"].shock_tests(
-            selected_model_ref,
-            matrices["X_val_ref"],
-            matrices["y_val_ref"]
-        )
-        
+            selected_model_ref, matrices["X_val_ref"], matrices["y_val_ref"]
+        )
+
         # OpenAI decision: Accept evaluation results
         decision_prompt = f"""
         Review the final evaluation and robustness results:
         
         Test metrics: {test_metrics}
@@ -429,48 +444,51 @@
         Decision needed:
         1. Are these results acceptable for deployment?
         2. Any concerns about model robustness?
         3. Final recommendations?
         """
-        
+
         decision = self.llm_router.openai_decide(
             stage="evaluation_stress",
             prompt=decision_prompt,
-            context={
-                "test_metrics": test_metrics,
-                "robustness": robustness_result
-            }
-        )
-        
+            context={"test_metrics": test_metrics, "robustness": robustness_result},
+        )
+
         return {
             "test_metrics": test_metrics,
             "robustness_result": robustness_result,
-            "evaluation_decision": decision
-        }
-    
+            "evaluation_decision": decision,
+        }
+
     def _stage_reporting(self) -> Dict[str, Any]:
         """Stage 7: Reporting & Packaging."""
         # Collect all results
         all_outputs = self.stage_outputs
-        
+
         # Prepare report payload
         report_payload = {
             "business_goal": self.config.business_goal,
             "primary_metric": self.config.metrics.get("primary", "auto"),
             "dataset_info": {"shape": "N/A"},  # Simplified for MVP
             "target": all_outputs["intake_validation"]["target"],
             "leakage_status": all_outputs["profiling_quality"]["leakage_status"],
             "top_insights": all_outputs["eda_hypotheses"]["top_insights"],
             "selected_model": all_outputs["model_ladder"]["selected_model"],
             "leaderboard": all_outputs["model_ladder"]["comparison"]["rankings"][:3],
-            "operating_point": {"threshold": "0.5", "conservative": "0.7", "tradeoff": "Balanced precision/recall"},
-            "robustness_grade": all_outputs["evaluation_stress"]["robustness_result"]["resilience_grade"],
+            "operating_point": {
+                "threshold": "0.5",
+                "conservative": "0.7",
+                "tradeoff": "Balanced precision/recall",
+            },
+            "robustness_grade": all_outputs["evaluation_stress"]["robustness_result"][
+                "resilience_grade"
+            ],
             "robustness_reason": "Based on shock test results",
             "next_steps": ["Deploy model", "Monitor performance", "Collect feedback"],
-            "shortcuts_taken": self.manifest.shortcuts_taken
-        }
-        
+            "shortcuts_taken": self.manifest.shortcuts_taken,
+        }
+
         # OpenAI decision: Final report sign-off
         decision_prompt = f"""
         Review the complete analysis and provide final sign-off for the executive summary:
         
         Results summary: {report_payload}
@@ -478,34 +496,32 @@
         Provide:
         1. Executive summary text
         2. Key recommendations
         3. Any caveats or limitations
         """
-        
+
         decision = self.llm_router.openai_decide(
             stage="reporting",
             prompt=decision_prompt,
-            context={"report_payload": report_payload}
-        )
-        
+            context={"report_payload": report_payload},
+        )
+
         # Generate reports
         one_pager = self.tools["artifact_store"].write_report(
-            "one_pager",
-            report_payload,
-            self.config.report.format
-        )
-        
+            "one_pager", report_payload, self.config.report.format
+        )
+
         appendix = self.tools["artifact_store"].write_report(
             "appendix",
             {
                 "config_snapshot": str(self.config.model_dump()),
                 "dataset_hash": "N/A",  # Simplified for MVP
-                "seeds": {"main": 42}
+                "seeds": {"main": 42},
             },
-            self.config.report.format
-        )
-        
+            self.config.report.format,
+        )
+
         return {
             "one_pager_ref": one_pager["report_ref"],
             "appendix_ref": appendix["report_ref"],
-            "final_decision": decision
-        }
+            "final_decision": decision,
+        }
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/profiling.py	2025-09-04 23:54:27.383626+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/profiling.py	2025-10-17 00:12:38.202495+00:00
@@ -9,111 +9,118 @@
 from ..utils import load_pickle, save_json
 
 
 class SchemaProfiler:
     """Dataset schema and basic profiling."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.handoffs_dir = artifacts_dir / "handoffs"
         self.handoffs_dir.mkdir(parents=True, exist_ok=True)
-    
+
     def profile(self, df_ref: str) -> Dict[str, Any]:
         """
         Generate comprehensive dataset profile.
-        
+
         Args:
             df_ref: Reference to pickled DataFrame
-            
+
         Returns:
             Profile dictionary saved to profile.json
         """
         df = load_pickle(df_ref)
-        
+
         # Basic shape and structure
         profile = {
-            "shape": {
-                "rows": len(df),
-                "columns": len(df.columns)
-            },
+            "shape": {"rows": len(df), "columns": len(df.columns)},
             "columns": list(df.columns),
             "dtypes": df.dtypes.astype(str).to_dict(),
-            "memory_usage_mb": df.memory_usage(deep=True).sum() / (1024 * 1024)
-        }
-        
+            "memory_usage_mb": df.memory_usage(deep=True).sum() / (1024 * 1024),
+        }
+
         # Missing values analysis
         missing = df.isnull().sum()
         profile["missing_values"] = {
             "total_missing": int(missing.sum()),
             "missing_by_column": missing[missing > 0].to_dict(),
-            "missing_percentage": (missing / len(df) * 100).round(2).to_dict()
-        }
-        
+            "missing_percentage": (missing / len(df) * 100).round(2).to_dict(),
+        }
+
         # Data type analysis
         numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
-        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
-        datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
-        
+        categorical_cols = df.select_dtypes(
+            include=["object", "category"]
+        ).columns.tolist()
+        datetime_cols = df.select_dtypes(include=["datetime64"]).columns.tolist()
+
         profile["column_types"] = {
             "numeric": numeric_cols,
             "categorical": categorical_cols,
-            "datetime": datetime_cols
-        }
-        
+            "datetime": datetime_cols,
+        }
+
         # Basic statistics for numeric columns
         if numeric_cols:
             numeric_stats = df[numeric_cols].describe().to_dict()
             profile["numeric_stats"] = numeric_stats
-        
+
         # Categorical column analysis
         if categorical_cols:
             cat_stats = {}
             for col in categorical_cols:
                 value_counts = df[col].value_counts()
                 cat_stats[col] = {
                     "unique_values": int(df[col].nunique()),
-                    "most_frequent": str(value_counts.index[0]) if len(value_counts) > 0 else None,
-                    "most_frequent_count": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
-                    "top_5_values": value_counts.head(5).to_dict()
+                    "most_frequent": (
+                        str(value_counts.index[0]) if len(value_counts) > 0 else None
+                    ),
+                    "most_frequent_count": (
+                        int(value_counts.iloc[0]) if len(value_counts) > 0 else 0
+                    ),
+                    "top_5_values": value_counts.head(5).to_dict(),
                 }
             profile["categorical_stats"] = cat_stats
-        
+
         # Potential target analysis (if target column exists)
         potential_targets = []
         for col in df.columns:
             unique_vals = df[col].nunique()
             if 2 <= unique_vals <= 20:  # Potential classification target
-                potential_targets.append({
-                    "column": col,
-                    "unique_values": unique_vals,
-                    "type": "classification_candidate"
-                })
-            elif df[col].dtype in ['int64', 'float64'] and unique_vals > 20:
-                potential_targets.append({
-                    "column": col,
-                    "unique_values": unique_vals,
-                    "type": "regression_candidate"
-                })
-        
+                potential_targets.append(
+                    {
+                        "column": col,
+                        "unique_values": unique_vals,
+                        "type": "classification_candidate",
+                    }
+                )
+            elif df[col].dtype in ["int64", "float64"] and unique_vals > 20:
+                potential_targets.append(
+                    {
+                        "column": col,
+                        "unique_values": unique_vals,
+                        "type": "regression_candidate",
+                    }
+                )
+
         profile["potential_targets"] = potential_targets
-        
+
         # Save profile
         profile_path = self.handoffs_dir / "profile.json"
         hash_value = save_json(profile, profile_path)
-        
+
         return {
             "profile_ref": str(profile_path),
             "hash": hash_value,
             "summary": {
                 "rows": profile["shape"]["rows"],
                 "columns": profile["shape"]["columns"],
                 "missing_total": profile["missing_values"]["total_missing"],
                 "numeric_columns": len(numeric_cols),
-                "categorical_columns": len(categorical_cols)
-            }
-        }
-    
+                "categorical_columns": len(categorical_cols),
+            },
+        }
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -123,140 +130,162 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference path to pickled DataFrame"
+                                "description": "Reference path to pickled DataFrame",
                             }
                         },
-                        "required": ["df_ref"]
-                    }
-                }
+                        "required": ["df_ref"],
+                    },
+                },
             }
         ]
 
 
 class QualityGuard:
     """Data quality and leakage detection."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
-    
+
     def leakage_scan(
         self,
         df_ref: str,
         target: str,
         split_policy: str,
-        time_col: Optional[str] = None
+        time_col: Optional[str] = None,
     ) -> Dict[str, Any]:
         """
         Scan for potential data leakage issues.
-        
+
         Args:
             df_ref: Reference to DataFrame
             target: Target column name
             split_policy: 'iid' or 'time'
             time_col: Time column for temporal splits
-            
+
         Returns:
             Leakage scan results
         """
         df = load_pickle(df_ref)
-        
+
         if target not in df.columns:
             return {
                 "status": "block",
                 "offenders": [],
-                "reason": f"Target column '{target}' not found in dataset"
+                "reason": f"Target column '{target}' not found in dataset",
             }
-        
+
         offenders = []
         warnings = []
-        
+
         # Check for perfect correlation with target
-        if df[target].dtype in ['int64', 'float64']:
+        if df[target].dtype in ["int64", "float64"]:
             for col in df.columns:
-                if col != target and df[col].dtype in ['int64', 'float64']:
+                if col != target and df[col].dtype in ["int64", "float64"]:
                     try:
                         corr = df[col].corr(df[target])
                         if abs(corr) > 0.99:
-                            offenders.append({
-                                "column": col,
-                                "issue": "perfect_correlation",
-                                "correlation": float(corr),
-                                "severity": "block"
-                            })
+                            offenders.append(
+                                {
+                                    "column": col,
+                                    "issue": "perfect_correlation",
+                                    "correlation": float(corr),
+                                    "severity": "block",
+                                }
+                            )
                         elif abs(corr) > 0.95:
-                            warnings.append({
-                                "column": col,
-                                "issue": "high_correlation",
-                                "correlation": float(corr),
-                                "severity": "warn"
-                            })
+                            warnings.append(
+                                {
+                                    "column": col,
+                                    "issue": "high_correlation",
+                                    "correlation": float(corr),
+                                    "severity": "warn",
+                                }
+                            )
                     except Exception:
                         pass
-        
+
         # Check for suspicious column names
         suspicious_patterns = [
-            'id', 'index', 'key', 'uuid', 'guid',
-            'created', 'updated', 'modified', 'timestamp',
-            'result', 'outcome', 'prediction', 'score',
-            'label', 'class', 'category'
+            "id",
+            "index",
+            "key",
+            "uuid",
+            "guid",
+            "created",
+            "updated",
+            "modified",
+            "timestamp",
+            "result",
+            "outcome",
+            "prediction",
+            "score",
+            "label",
+            "class",
+            "category",
         ]
-        
+
         for col in df.columns:
             if col != target:
                 col_lower = col.lower()
                 for pattern in suspicious_patterns:
                     if pattern in col_lower:
-                        warnings.append({
-                            "column": col,
-                            "issue": "suspicious_name",
-                            "pattern": pattern,
-                            "severity": "warn"
-                        })
+                        warnings.append(
+                            {
+                                "column": col,
+                                "issue": "suspicious_name",
+                                "pattern": pattern,
+                                "severity": "warn",
+                            }
+                        )
                         break
-        
+
         # Time-based leakage checks
         if split_policy == "time" and time_col:
             if time_col not in df.columns:
-                offenders.append({
-                    "column": time_col,
-                    "issue": "missing_time_column",
-                    "severity": "block"
-                })
+                offenders.append(
+                    {
+                        "column": time_col,
+                        "issue": "missing_time_column",
+                        "severity": "block",
+                    }
+                )
             else:
                 # Check for future information
                 try:
                     df[time_col] = pd.to_datetime(df[time_col])
                     # Additional time-based checks could go here
                 except Exception:
-                    warnings.append({
-                        "column": time_col,
-                        "issue": "invalid_datetime_format",
-                        "severity": "warn"
-                    })
-        
+                    warnings.append(
+                        {
+                            "column": time_col,
+                            "issue": "invalid_datetime_format",
+                            "severity": "warn",
+                        }
+                    )
+
         # Determine overall status
         if offenders:
             status = "block"
         elif warnings:
             status = "warn"
         else:
             status = "pass"
-        
+
         return {
             "status": status,
             "offenders": offenders,
             "warnings": warnings,
             "summary": {
                 "blocking_issues": len(offenders),
                 "warnings": len(warnings),
-                "columns_checked": len(df.columns) - 1  # Exclude target
-            }
-        }
-    
+                "columns_checked": len(df.columns) - 1,  # Exclude target
+            },
+        }
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -266,26 +295,26 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "df_ref": {
                                 "type": "string",
-                                "description": "Reference to pickled DataFrame"
+                                "description": "Reference to pickled DataFrame",
                             },
                             "target": {
                                 "type": "string",
-                                "description": "Target column name"
+                                "description": "Target column name",
                             },
                             "split_policy": {
                                 "type": "string",
                                 "enum": ["iid", "time"],
-                                "description": "Data splitting policy"
+                                "description": "Data splitting policy",
                             },
                             "time_col": {
                                 "type": "string",
-                                "description": "Time column name for temporal splits (optional)"
-                            }
+                                "description": "Time column name for temporal splits (optional)",
+                            },
                         },
-                        "required": ["df_ref", "target", "split_policy"]
-                    }
-                }
+                        "required": ["df_ref", "target", "split_policy"],
+                    },
+                },
             }
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/pipeline.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_cross_dataset.py	2025-09-05 00:04:54.128329+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_cross_dataset.py	2025-10-17 00:12:38.211920+00:00
@@ -12,115 +12,117 @@
 from botds import Config, Pipeline
 
 
 class TestCrossDataset(unittest.TestCase):
     """Test pipeline on all three datasets."""
-    
+
     def setUp(self):
         """Set up test environment."""
         # Skip tests if no OpenAI API key
         if not os.getenv("OPENAI_API_KEY"):
             self.skipTest("OPENAI_API_KEY not set")
-        
+
         # Create temporary directory for artifacts
         self.temp_dir = tempfile.mkdtemp()
-    
+
     def test_iris_dataset(self):
         """Test pipeline on Iris dataset."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
-        
+
         # Override output directory
         config.report.out_dir = self.temp_dir
-        
+
         # Run pipeline
         pipeline = Pipeline(config)
         result = pipeline.run()
-        
+
         # Check success
         self.assertEqual(result["status"], "success")
         self.assertIn("job_id", result)
-        
+
         # Check artifacts exist
         artifacts_dir = Path(result["artifacts_dir"])
         self.assertTrue(artifacts_dir.exists())
-        
+
         # Check key files exist
         expected_files = [
             "handoffs/profile.json",
-            "handoffs/split_indices.json", 
+            "handoffs/split_indices.json",
             "handoffs/feature_plan.json",
             "reports/one_pager.html",
             "logs/decision_log.jsonl",
-            "run_manifest.json"
+            "run_manifest.json",
         ]
-        
+
         for file_path in expected_files:
             full_path = artifacts_dir / file_path
             self.assertTrue(full_path.exists(), f"Missing file: {file_path}")
-    
+
     def test_breast_cancer_dataset(self):
         """Test pipeline on Breast Cancer dataset."""
         config_path = Path(__file__).parent.parent / "configs" / "breast_cancer.yaml"
         config = Config.from_yaml(str(config_path))
-        
+
         # Override output directory
         config.report.out_dir = self.temp_dir
-        
+
         # Run pipeline
         pipeline = Pipeline(config)
         result = pipeline.run()
-        
+
         # Check success
         self.assertEqual(result["status"], "success")
-        
+
         # Check primary metric is pr_auc
         self.assertEqual(config.metrics["primary"], "pr_auc")
-    
+
     def test_diabetes_dataset(self):
         """Test pipeline on Diabetes dataset (regression)."""
         config_path = Path(__file__).parent.parent / "configs" / "diabetes.yaml"
         config = Config.from_yaml(str(config_path))
-        
+
         # Override output directory
         config.report.out_dir = self.temp_dir
-        
+
         # Run pipeline
         pipeline = Pipeline(config)
         result = pipeline.run()
-        
+
         # Check success
         self.assertEqual(result["status"], "success")
-        
+
         # Check primary metric is mae
         self.assertEqual(config.metrics["primary"], "mae")
-    
+
     def test_all_datasets_produce_reports(self):
         """Test that all datasets produce complete reports."""
         datasets = ["iris", "breast_cancer", "diabetes"]
-        
+
         for dataset in datasets:
             with self.subTest(dataset=dataset):
-                config_path = Path(__file__).parent.parent / "configs" / f"{dataset}.yaml"
+                config_path = (
+                    Path(__file__).parent.parent / "configs" / f"{dataset}.yaml"
+                )
                 config = Config.from_yaml(str(config_path))
                 config.report.out_dir = self.temp_dir
-                
+
                 pipeline = Pipeline(config)
                 result = pipeline.run()
-                
+
                 self.assertEqual(result["status"], "success")
-                
+
                 # Check report files
                 artifacts_dir = Path(result["artifacts_dir"])
                 one_pager = artifacts_dir / "reports" / "one_pager.html"
                 appendix = artifacts_dir / "reports" / "appendix.html"
-                
+
                 self.assertTrue(one_pager.exists())
                 self.assertTrue(appendix.exists())
-                
+
                 # Check report content is not empty
                 self.assertGreater(one_pager.stat().st_size, 1000)  # At least 1KB
-                self.assertGreater(appendix.stat().st_size, 100)   # At least 100B
+                self.assertGreater(appendix.stat().st_size, 100)  # At least 100B
 
 
 if __name__ == "__main__":
     unittest.main()
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/profiling.py
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_cross_dataset.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_cache_and_invalidation.py	2025-09-05 00:05:21.625843+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_cache_and_invalidation.py	2025-10-17 00:12:38.222485+00:00
@@ -13,132 +13,144 @@
 from botds.cache import Cache
 
 
 class TestCacheAndInvalidation(unittest.TestCase):
     """Test cache behavior and invalidation logic."""
-    
+
     def setUp(self):
         """Set up test environment."""
         if not os.getenv("OPENAI_API_KEY"):
             self.skipTest("OPENAI_API_KEY not set")
-        
+
         self.temp_dir = tempfile.mkdtemp()
         self.cache_dir = Path(self.temp_dir) / "cache"
-    
+
     def test_cache_warm_rerun(self):
         """Test that warm cache reuses results."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
-        
+
         # Set cache and output directories
         config.cache.dir = str(self.cache_dir)
         config.cache.mode = "warm"
         config.report.out_dir = self.temp_dir
-        
+
         # First run
         pipeline1 = Pipeline(config)
         result1 = pipeline1.run()
         self.assertEqual(result1["status"], "success")
-        
+
         cache_stats1 = result1.get("cache_stats", {})
-        
+
         # Second run (should hit cache)
         pipeline2 = Pipeline(config)
         result2 = pipeline2.run()
         self.assertEqual(result2["status"], "success")
-        
+
         cache_stats2 = result2.get("cache_stats", {})
-        
+
         # Check that second run had more cache hits
         hits1 = sum(1 for hit in cache_stats1.values() if hit)
         hits2 = sum(1 for hit in cache_stats2.values() if hit)
-        
+
         # Second run should have more hits (at least some stages cached)
         self.assertGreaterEqual(hits2, hits1)
-    
+
     def test_cache_cold_mode(self):
         """Test that cold cache ignores existing cache."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
-        
+
         config.cache.dir = str(self.cache_dir)
         config.report.out_dir = self.temp_dir
-        
+
         # First run with warm cache
         config.cache.mode = "warm"
         pipeline1 = Pipeline(config)
         result1 = pipeline1.run()
         self.assertEqual(result1["status"], "success")
-        
+
         # Second run with cold cache
         config.cache.mode = "cold"
         pipeline2 = Pipeline(config)
         result2 = pipeline2.run()
         self.assertEqual(result2["status"], "success")
-        
+
         cache_stats2 = result2.get("cache_stats", {})
-        
+
         # Cold mode should have no cache hits
         hits2 = sum(1 for hit in cache_stats2.values() if hit)
         self.assertEqual(hits2, 0)
-    
+
     def test_cache_invalidation_logic(self):
         """Test cache invalidation dependencies."""
         cache = Cache(str(self.cache_dir), "warm")
-        
+
         # Put some test data
         cache.put("profile", "test_key", {"test": "data"}, [])
         cache.put("eda", "test_key", {"eda": "data"}, ["profile:test_key"])
-        cache.put("feature_plan", "test_key", {"features": "data"}, ["profile:test_key", "eda:test_key"])
-        
+        cache.put(
+            "feature_plan",
+            "test_key",
+            {"features": "data"},
+            ["profile:test_key", "eda:test_key"],
+        )
+
         # Check initial state
         self.assertIsNotNone(cache.get("profile", "test_key"))
         self.assertIsNotNone(cache.get("eda", "test_key"))
         self.assertIsNotNone(cache.get("feature_plan", "test_key"))
-        
+
         # Invalidate profile stage
         invalidated = cache.invalidate_downstream("profile")
-        
+
         # Should invalidate downstream stages
-        expected_invalidated = {"eda", "feature_plan", "split_indices", "ladder", "evaluation", "reports"}
+        expected_invalidated = {
+            "eda",
+            "feature_plan",
+            "split_indices",
+            "ladder",
+            "evaluation",
+            "reports",
+        }
         self.assertTrue(invalidated.intersection(expected_invalidated))
-    
+
     def test_cache_index_persistence(self):
         """Test that cache index persists across instances."""
         cache1 = Cache(str(self.cache_dir), "warm")
-        
+
         # Put data in first cache instance
         cache1.put("profile", "test_key", {"test": "data"})
-        
+
         # Create new cache instance
         cache2 = Cache(str(self.cache_dir), "warm")
-        
+
         # Should be able to retrieve data
         data = cache2.get("profile", "test_key")
         self.assertIsNotNone(data)
         self.assertEqual(data["test"], "data")
-    
+
     def test_cache_hit_miss_tracking(self):
         """Test cache hit/miss statistics tracking."""
         cache = Cache(str(self.cache_dir), "warm")
-        
+
         # Miss on empty cache
         result1 = cache.get("profile", "missing_key")
         self.assertIsNone(result1)
-        
+
         # Put data
         cache.put("profile", "existing_key", {"data": "value"})
-        
+
         # Hit on existing data
         result2 = cache.get("profile", "existing_key")
         self.assertIsNotNone(result2)
-        
+
         # Check hit/miss stats
         stats = cache.get_hit_stats()
         self.assertIn("profile:missing_key", stats)
         self.assertIn("profile:existing_key", stats)
         self.assertFalse(stats["profile:missing_key"])  # Miss
-        self.assertTrue(stats["profile:existing_key"])   # Hit
+        self.assertTrue(stats["profile:existing_key"])  # Hit
 
 
 if __name__ == "__main__":
     unittest.main()
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_cache_and_invalidation.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/plotter.py	2025-09-04 23:58:22.703435+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/plotter.py	2025-10-17 00:12:38.226269+00:00
@@ -11,320 +11,333 @@
 from ..utils import load_pickle
 
 
 class Plotter:
     """Visualization and plotting tools."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.plots_dir = artifacts_dir / "plots"
         self.plots_dir.mkdir(parents=True, exist_ok=True)
-        
+
         # Set matplotlib style
-        plt.style.use('default')
-        plt.rcParams['figure.figsize'] = (8, 6)
-        plt.rcParams['font.size'] = 10
-    
+        plt.style.use("default")
+        plt.rcParams["figure.figsize"] = (8, 6)
+        plt.rcParams["font.size"] = 10
+
     def pr_curve(
         self,
         model_ref: str,
         X_test_ref: str,
         y_test_ref: str,
-        title: Optional[str] = None
+        title: Optional[str] = None,
     ) -> Dict[str, Any]:
         """
         Plot Precision-Recall curve.
-        
+
         Args:
             model_ref: Reference to trained model
             X_test_ref: Test features reference
             y_test_ref: Test target reference
             title: Plot title
-            
+
         Returns:
             Plot file reference and metrics
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_test = load_pickle(X_test_ref)
         y_test = load_pickle(y_test_ref)
-        
+
         # Check if binary classification
         if len(np.unique(y_test)) != 2:
             return {
                 "plot_ref": None,
-                "error": "PR curve only available for binary classification"
-            }
-        
+                "error": "PR curve only available for binary classification",
+            }
+
         # Check if model supports probabilities
         if not hasattr(model, "predict_proba"):
             return {
                 "plot_ref": None,
-                "error": "Model does not support probability prediction"
-            }
-        
+                "error": "Model does not support probability prediction",
+            }
+
         # Get probabilities
         y_proba = model.predict_proba(X_test)[:, 1]
-        
+
         # Calculate PR curve
         precision, recall, thresholds = precision_recall_curve(y_test, y_proba)
-        
+
         # Calculate AUC
         from sklearn.metrics import average_precision_score
+
         pr_auc = average_precision_score(y_test, y_proba)
-        
+
         # Create plot
         plt.figure(figsize=(8, 6))
-        plt.plot(recall, precision, linewidth=2, label=f'PR Curve (AUC = {pr_auc:.3f})')
-        plt.xlabel('Recall')
-        plt.ylabel('Precision')
-        plt.title(title or 'Precision-Recall Curve')
+        plt.plot(recall, precision, linewidth=2, label=f"PR Curve (AUC = {pr_auc:.3f})")
+        plt.xlabel("Recall")
+        plt.ylabel("Precision")
+        plt.title(title or "Precision-Recall Curve")
         plt.legend()
         plt.grid(True, alpha=0.3)
-        
+
         # Save plot
         plot_path = self.plots_dir / "pr_curve.png"
-        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
+        plt.savefig(plot_path, dpi=150, bbox_inches="tight")
         plt.close()
-        
+
         return {
             "plot_ref": str(plot_path),
             "pr_auc": float(pr_auc),
-            "optimal_threshold": float(thresholds[np.argmax(precision + recall)])
+            "optimal_threshold": float(thresholds[np.argmax(precision + recall)]),
         }
-    
+
     def lift_curve(
         self,
         model_ref: str,
         X_test_ref: str,
         y_test_ref: str,
-        title: Optional[str] = None
+        title: Optional[str] = None,
     ) -> Dict[str, Any]:
         """
         Plot lift curve.
-        
+
         Args:
             model_ref: Reference to trained model
             X_test_ref: Test features reference
             y_test_ref: Test target reference
             title: Plot title
-            
+
         Returns:
             Plot file reference and lift metrics
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_test = load_pickle(X_test_ref)
         y_test = load_pickle(y_test_ref)
-        
+
         # Check if binary classification
         if len(np.unique(y_test)) != 2:
             return {
                 "plot_ref": None,
-                "error": "Lift curve only available for binary classification"
-            }
-        
+                "error": "Lift curve only available for binary classification",
+            }
+
         # Check if model supports probabilities
         if not hasattr(model, "predict_proba"):
             return {
                 "plot_ref": None,
-                "error": "Model does not support probability prediction"
-            }
-        
+                "error": "Model does not support probability prediction",
+            }
+
         # Get probabilities and sort by descending probability
         y_proba = model.predict_proba(X_test)[:, 1]
         sorted_indices = np.argsort(y_proba)[::-1]
-        y_test_sorted = y_test.iloc[sorted_indices] if hasattr(y_test, 'iloc') else y_test[sorted_indices]
-        
+        y_test_sorted = (
+            y_test.iloc[sorted_indices]
+            if hasattr(y_test, "iloc")
+            else y_test[sorted_indices]
+        )
+
         # Calculate lift
         n_samples = len(y_test)
         n_positives = y_test.sum()
         baseline_rate = n_positives / n_samples
-        
+
         percentiles = np.arange(0.1, 1.1, 0.1)
         lift_values = []
-        
+
         for pct in percentiles:
             n_top = int(pct * n_samples)
             if n_top == 0:
                 continue
-            
+
             top_positives = y_test_sorted[:n_top].sum()
             top_rate = top_positives / n_top
             lift = top_rate / baseline_rate if baseline_rate > 0 else 1
             lift_values.append(lift)
-        
+
         # Create plot
         plt.figure(figsize=(8, 6))
-        plt.plot(percentiles[:len(lift_values)], lift_values, 'b-', linewidth=2, marker='o')
-        plt.axhline(y=1, color='r', linestyle='--', alpha=0.7, label='Baseline')
-        plt.xlabel('Population Percentile')
-        plt.ylabel('Lift')
-        plt.title(title or 'Lift Curve')
+        plt.plot(
+            percentiles[: len(lift_values)], lift_values, "b-", linewidth=2, marker="o"
+        )
+        plt.axhline(y=1, color="r", linestyle="--", alpha=0.7, label="Baseline")
+        plt.xlabel("Population Percentile")
+        plt.ylabel("Lift")
+        plt.title(title or "Lift Curve")
         plt.legend()
         plt.grid(True, alpha=0.3)
-        
+
         # Save plot
         plot_path = self.plots_dir / "lift_curve.png"
-        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
+        plt.savefig(plot_path, dpi=150, bbox_inches="tight")
         plt.close()
-        
+
         return {
             "plot_ref": str(plot_path),
             "max_lift": float(max(lift_values)) if lift_values else 1.0,
-            "lift_at_10pct": float(lift_values[0]) if lift_values else 1.0
+            "lift_at_10pct": float(lift_values[0]) if lift_values else 1.0,
         }
-    
+
     def calibration_plot(
         self,
         model_ref: str,
         X_test_ref: str,
         y_test_ref: str,
-        title: Optional[str] = None
+        title: Optional[str] = None,
     ) -> Dict[str, Any]:
         """
         Plot calibration plot.
-        
+
         Args:
             model_ref: Reference to trained model
             X_test_ref: Test features reference
             y_test_ref: Test target reference
             title: Plot title
-            
+
         Returns:
             Plot file reference and calibration metrics
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_test = load_pickle(X_test_ref)
         y_test = load_pickle(y_test_ref)
-        
+
         # Check if binary classification
         if len(np.unique(y_test)) != 2:
             return {
                 "plot_ref": None,
-                "error": "Calibration plot only available for binary classification"
-            }
-        
+                "error": "Calibration plot only available for binary classification",
+            }
+
         # Check if model supports probabilities
         if not hasattr(model, "predict_proba"):
             return {
                 "plot_ref": None,
-                "error": "Model does not support probability prediction"
-            }
-        
+                "error": "Model does not support probability prediction",
+            }
+
         # Get probabilities
         y_proba = model.predict_proba(X_test)[:, 1]
-        
+
         # Calculate calibration curve
         n_bins = 10
         bin_boundaries = np.linspace(0, 1, n_bins + 1)
         bin_lowers = bin_boundaries[:-1]
         bin_uppers = bin_boundaries[1:]
-        
+
         bin_centers = []
         bin_accuracies = []
         bin_counts = []
-        
+
         for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
             in_bin = (y_proba >= bin_lower) & (y_proba < bin_upper)
             if bin_upper == 1.0:  # Include the last bin edge
                 in_bin = (y_proba >= bin_lower) & (y_proba <= bin_upper)
-            
+
             if in_bin.sum() > 0:
                 bin_centers.append((bin_lower + bin_upper) / 2)
                 bin_accuracies.append(y_test[in_bin].mean())
                 bin_counts.append(in_bin.sum())
-        
+
         # Create plot
         plt.figure(figsize=(8, 6))
-        plt.plot([0, 1], [0, 1], 'k--', alpha=0.7, label='Perfect Calibration')
-        
+        plt.plot([0, 1], [0, 1], "k--", alpha=0.7, label="Perfect Calibration")
+
         if bin_centers:
-            plt.plot(bin_centers, bin_accuracies, 'bo-', linewidth=2, markersize=8, label='Model')
-            
+            plt.plot(
+                bin_centers,
+                bin_accuracies,
+                "bo-",
+                linewidth=2,
+                markersize=8,
+                label="Model",
+            )
+
             # Add bin counts as text
             for x, y, count in zip(bin_centers, bin_accuracies, bin_counts):
-                plt.text(x, y + 0.02, str(count), ha='center', va='bottom', fontsize=8)
-        
-        plt.xlabel('Mean Predicted Probability')
-        plt.ylabel('Fraction of Positives')
-        plt.title(title or 'Calibration Plot')
+                plt.text(x, y + 0.02, str(count), ha="center", va="bottom", fontsize=8)
+
+        plt.xlabel("Mean Predicted Probability")
+        plt.ylabel("Fraction of Positives")
+        plt.title(title or "Calibration Plot")
         plt.legend()
         plt.grid(True, alpha=0.3)
-        
+
         # Save plot
         plot_path = self.plots_dir / "calibration_plot.png"
-        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
+        plt.savefig(plot_path, dpi=150, bbox_inches="tight")
         plt.close()
-        
+
         # Calculate Expected Calibration Error
         ece = 0
         total_samples = len(y_test)
         for center, accuracy, count in zip(bin_centers, bin_accuracies, bin_counts):
             ece += abs(center - accuracy) * (count / total_samples)
-        
+
         return {
             "plot_ref": str(plot_path),
             "ece": float(ece),
-            "n_bins": len(bin_centers)
+            "n_bins": len(bin_centers),
         }
-    
+
     def bars(
         self,
         data: Dict[str, float],
         title: str,
         xlabel: str = "Categories",
-        ylabel: str = "Values"
+        ylabel: str = "Values",
     ) -> Dict[str, Any]:
         """
         Create bar plot.
-        
+
         Args:
             data: Dictionary of category -> value
             title: Plot title
             xlabel: X-axis label
             ylabel: Y-axis label
-            
+
         Returns:
             Plot file reference
         """
         if not data:
-            return {
-                "plot_ref": None,
-                "error": "No data provided for bar plot"
-            }
-        
+            return {"plot_ref": None, "error": "No data provided for bar plot"}
+
         # Create plot
         plt.figure(figsize=(10, 6))
         categories = list(data.keys())
         values = list(data.values())
-        
+
         bars = plt.bar(categories, values)
         plt.xlabel(xlabel)
         plt.ylabel(ylabel)
         plt.title(title)
-        plt.xticks(rotation=45, ha='right')
-        
+        plt.xticks(rotation=45, ha="right")
+
         # Add value labels on bars
         for bar, value in zip(bars, values):
-            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(values)*0.01,
-                    f'{value:.3f}', ha='center', va='bottom')
-        
+            plt.text(
+                bar.get_x() + bar.get_width() / 2,
+                bar.get_height() + max(values) * 0.01,
+                f"{value:.3f}",
+                ha="center",
+                va="bottom",
+            )
+
         plt.tight_layout()
-        
+
         # Save plot
-        plot_name = title.lower().replace(' ', '_').replace('/', '_') + ".png"
+        plot_name = title.lower().replace(" ", "_").replace("/", "_") + ".png"
         plot_path = self.plots_dir / plot_name
-        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
+        plt.savefig(plot_path, dpi=150, bbox_inches="tight")
         plt.close()
-        
-        return {
-            "plot_ref": str(plot_path),
-            "n_categories": len(categories)
-        }
-    
+
+        return {"plot_ref": str(plot_path), "n_categories": len(categories)}
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -334,28 +347,28 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test features"
+                                "description": "Reference to test features",
                             },
                             "y_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test target"
+                                "description": "Reference to test target",
                             },
                             "title": {
                                 "type": "string",
-                                "description": "Plot title (optional)"
-                            }
+                                "description": "Plot title (optional)",
+                            },
                         },
-                        "required": ["model_ref", "X_test_ref", "y_test_ref"]
-                    }
-                }
+                        "required": ["model_ref", "X_test_ref", "y_test_ref"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "Plotter_lift_curve",
@@ -363,28 +376,28 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test features"
+                                "description": "Reference to test features",
                             },
                             "y_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test target"
+                                "description": "Reference to test target",
                             },
                             "title": {
                                 "type": "string",
-                                "description": "Plot title (optional)"
-                            }
+                                "description": "Plot title (optional)",
+                            },
                         },
-                        "required": ["model_ref", "X_test_ref", "y_test_ref"]
-                    }
-                }
+                        "required": ["model_ref", "X_test_ref", "y_test_ref"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "Plotter_calibration_plot",
@@ -392,28 +405,28 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test features"
+                                "description": "Reference to test features",
                             },
                             "y_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test target"
+                                "description": "Reference to test target",
                             },
                             "title": {
                                 "type": "string",
-                                "description": "Plot title (optional)"
-                            }
+                                "description": "Plot title (optional)",
+                            },
                         },
-                        "required": ["model_ref", "X_test_ref", "y_test_ref"]
-                    }
-                }
+                        "required": ["model_ref", "X_test_ref", "y_test_ref"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "Plotter_bars",
@@ -421,25 +434,22 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "data": {
                                 "type": "object",
-                                "description": "Dictionary mapping categories to values"
-                            },
-                            "title": {
-                                "type": "string",
-                                "description": "Plot title"
-                            },
+                                "description": "Dictionary mapping categories to values",
+                            },
+                            "title": {"type": "string", "description": "Plot title"},
                             "xlabel": {
                                 "type": "string",
-                                "description": "X-axis label (default: Categories)"
+                                "description": "X-axis label (default: Categories)",
                             },
                             "ylabel": {
                                 "type": "string",
-                                "description": "Y-axis label (default: Values)"
-                            }
+                                "description": "Y-axis label (default: Values)",
+                            },
                         },
-                        "required": ["data", "title"]
-                    }
-                }
-            }
+                        "required": ["data", "title"],
+                    },
+                },
+            },
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/plotter.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/eval.py	2025-09-04 23:57:32.156329+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/eval.py	2025-10-17 00:12:38.227056+00:00
@@ -11,101 +11,99 @@
 from ..utils import load_pickle, save_pickle
 
 
 class Calibrator:
     """Model calibration tools."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
         self.models_dir = artifacts_dir / "models"
         self.models_dir.mkdir(parents=True, exist_ok=True)
-    
+
     def fit(
-        self,
-        model_ref: str,
-        X_val_ref: str,
-        y_val_ref: str,
-        method: str = "isotonic"
+        self, model_ref: str, X_val_ref: str, y_val_ref: str, method: str = "isotonic"
     ) -> Dict[str, Any]:
         """
         Fit calibration on validation set.
-        
+
         Args:
             model_ref: Reference to trained model
             X_val_ref: Validation features reference
             y_val_ref: Validation target reference
             method: Calibration method ('isotonic' or 'sigmoid')
-            
+
         Returns:
             Calibrated model reference and calibration metrics
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_val = load_pickle(X_val_ref)
         y_val = load_pickle(y_val_ref)
-        
+
         # Check if model supports probability prediction
         if not hasattr(model, "predict_proba"):
             return {
                 "calibrated_model_ref": model_ref,  # Return original
                 "calibration_applied": False,
-                "reason": "Model does not support probability prediction"
+                "reason": "Model does not support probability prediction",
             }
-        
+
         # Fit calibration
         calibrated_model = CalibratedClassifierCV(model, method=method, cv="prefit")
         calibrated_model.fit(X_val, y_val)
-        
+
         # Save calibrated model
         cal_model_id = f"calibrated_{hash(model_ref)}_{method}"[:16]
         cal_model_path = self.models_dir / f"{cal_model_id}.pkl"
         save_pickle(calibrated_model, cal_model_path)
-        
+
         # Calculate calibration metrics
         y_proba_orig = model.predict_proba(X_val)[:, 1]
         y_proba_cal = calibrated_model.predict_proba(X_val)[:, 1]
-        
+
         # Expected Calibration Error (ECE)
         ece_orig = self._calculate_ece(y_val, y_proba_orig)
         ece_cal = self._calculate_ece(y_val, y_proba_cal)
-        
+
         # Brier Score
         brier_orig = brier_score_loss(y_val, y_proba_orig)
         brier_cal = brier_score_loss(y_val, y_proba_cal)
-        
+
         return {
             "calibrated_model_ref": str(cal_model_path),
             "calibration_applied": True,
             "method": method,
             "metrics": {
                 "ece_before": float(ece_orig),
                 "ece_after": float(ece_cal),
                 "ece_improvement": float(ece_orig - ece_cal),
                 "brier_before": float(brier_orig),
                 "brier_after": float(brier_cal),
-                "brier_improvement": float(brier_orig - brier_cal)
-            }
+                "brier_improvement": float(brier_orig - brier_cal),
+            },
         }
-    
-    def _calculate_ece(self, y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> float:
+
+    def _calculate_ece(
+        self, y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10
+    ) -> float:
         """Calculate Expected Calibration Error."""
         bin_boundaries = np.linspace(0, 1, n_bins + 1)
         bin_lowers = bin_boundaries[:-1]
         bin_uppers = bin_boundaries[1:]
-        
+
         ece = 0
         for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
             in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)
             prop_in_bin = in_bin.mean()
-            
+
             if prop_in_bin > 0:
                 accuracy_in_bin = y_true[in_bin].mean()
                 avg_confidence_in_bin = y_prob[in_bin].mean()
                 ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
-        
+
         return ece
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -115,168 +113,173 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation features"
+                                "description": "Reference to validation features",
                             },
                             "y_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation target"
+                                "description": "Reference to validation target",
                             },
                             "method": {
                                 "type": "string",
                                 "enum": ["isotonic", "sigmoid"],
-                                "description": "Calibration method (default: isotonic)"
-                            }
+                                "description": "Calibration method (default: isotonic)",
+                            },
                         },
-                        "required": ["model_ref", "X_val_ref", "y_val_ref"]
-                    }
-                }
+                        "required": ["model_ref", "X_val_ref", "y_val_ref"],
+                    },
+                },
             }
         ]
 
 
 class Fairness:
     """Fairness evaluation tools."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
-    
+
     def slice_metrics(
         self,
         model_ref: str,
         X_test_ref: str,
         y_test_ref: str,
-        sensitive_cols: List[str]
+        sensitive_cols: List[str],
     ) -> Dict[str, Any]:
         """
         Calculate metrics across sensitive attribute slices.
-        
+
         Args:
             model_ref: Reference to trained model
             X_test_ref: Test features reference
             y_test_ref: Test target reference
             sensitive_cols: List of sensitive column names
-            
+
         Returns:
             Per-slice metrics analysis
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_test = load_pickle(X_test_ref)
         y_test = load_pickle(y_test_ref)
-        
+
         # Get predictions
         y_pred = model.predict(X_test)
-        
+
         # Check which sensitive columns exist
         available_cols = [col for col in sensitive_cols if col in X_test.columns]
-        
+
         if not available_cols:
             return {
                 "slice_metrics": {},
                 "fairness_summary": {
                     "status": "no_sensitive_columns",
-                    "message": "No sensitive columns found in test data"
-                }
+                    "message": "No sensitive columns found in test data",
+                },
             }
-        
+
         slice_results = {}
-        
+
         for col in available_cols:
             col_results = {}
             unique_values = X_test[col].unique()
-            
+
             for value in unique_values:
                 mask = X_test[col] == value
                 if mask.sum() == 0:
                     continue
-                
+
                 y_true_slice = y_test[mask]
                 y_pred_slice = y_pred[mask]
-                
+
                 # Calculate metrics for this slice
                 if len(np.unique(y_true_slice)) <= 20:  # Classification
                     from sklearn.metrics import accuracy_score, f1_score
+
                     metrics = {
                         "accuracy": float(accuracy_score(y_true_slice, y_pred_slice)),
-                        "f1_score": float(f1_score(y_true_slice, y_pred_slice, average='weighted')),
-                        "sample_size": int(mask.sum())
+                        "f1_score": float(
+                            f1_score(y_true_slice, y_pred_slice, average="weighted")
+                        ),
+                        "sample_size": int(mask.sum()),
                     }
                 else:  # Regression
                     from sklearn.metrics import mean_absolute_error
+
                     metrics = {
                         "mae": float(mean_absolute_error(y_true_slice, y_pred_slice)),
-                        "sample_size": int(mask.sum())
+                        "sample_size": int(mask.sum()),
                     }
-                
+
                 col_results[str(value)] = metrics
-            
+
             slice_results[col] = col_results
-        
+
         # Calculate fairness summary
         fairness_summary = self._analyze_fairness(slice_results)
-        
-        return {
-            "slice_metrics": slice_results,
-            "fairness_summary": fairness_summary
-        }
-    
+
+        return {"slice_metrics": slice_results, "fairness_summary": fairness_summary}
+
     def _analyze_fairness(self, slice_results: Dict[str, Any]) -> Dict[str, Any]:
         """Analyze fairness across slices."""
         issues = []
-        
+
         for col, col_results in slice_results.items():
             if len(col_results) < 2:
                 continue
-            
+
             # Get primary metric values
             metric_values = []
             for slice_name, metrics in col_results.items():
                 if "accuracy" in metrics:
                     metric_values.append(("accuracy", metrics["accuracy"]))
                 elif "mae" in metrics:
                     metric_values.append(("mae", metrics["mae"]))
-            
+
             if len(metric_values) >= 2:
                 metric_name = metric_values[0][0]
                 values = [v[1] for v in metric_values]
-                
+
                 # Calculate disparity
                 max_val = max(values)
                 min_val = min(values)
-                
+
                 if metric_name == "accuracy":
                     disparity = max_val - min_val
                     if disparity > 0.1:  # 10% accuracy gap
-                        issues.append({
-                            "column": col,
-                            "metric": metric_name,
-                            "disparity": float(disparity),
-                            "severity": "high" if disparity > 0.2 else "medium"
-                        })
+                        issues.append(
+                            {
+                                "column": col,
+                                "metric": metric_name,
+                                "disparity": float(disparity),
+                                "severity": "high" if disparity > 0.2 else "medium",
+                            }
+                        )
                 elif metric_name == "mae":
-                    disparity = max_val / min_val if min_val > 0 else float('inf')
+                    disparity = max_val / min_val if min_val > 0 else float("inf")
                     if disparity > 1.5:  # 50% relative difference
-                        issues.append({
-                            "column": col,
-                            "metric": metric_name,
-                            "disparity_ratio": float(disparity),
-                            "severity": "high" if disparity > 2.0 else "medium"
-                        })
-        
+                        issues.append(
+                            {
+                                "column": col,
+                                "metric": metric_name,
+                                "disparity_ratio": float(disparity),
+                                "severity": "high" if disparity > 2.0 else "medium",
+                            }
+                        )
+
         return {
             "status": "issues_found" if issues else "no_issues",
             "issues": issues,
-            "total_issues": len(issues)
+            "total_issues": len(issues),
         }
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -286,266 +289,302 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test features"
+                                "description": "Reference to test features",
                             },
                             "y_test_ref": {
                                 "type": "string",
-                                "description": "Reference to test target"
+                                "description": "Reference to test target",
                             },
                             "sensitive_cols": {
                                 "type": "array",
                                 "items": {"type": "string"},
-                                "description": "List of sensitive column names"
-                            }
+                                "description": "List of sensitive column names",
+                            },
                         },
-                        "required": ["model_ref", "X_test_ref", "y_test_ref", "sensitive_cols"]
-                    }
-                }
+                        "required": [
+                            "model_ref",
+                            "X_test_ref",
+                            "y_test_ref",
+                            "sensitive_cols",
+                        ],
+                    },
+                },
             }
         ]
 
 
 class Robustness:
     """Model robustness evaluation."""
-    
+
     def __init__(self, artifacts_dir: Path):
         self.artifacts_dir = artifacts_dir
-    
+
     def ablation(
-        self,
-        model_ref: str,
-        X_val_ref: str,
-        y_val_ref: str,
-        top_k: int = 5
+        self, model_ref: str, X_val_ref: str, y_val_ref: str, top_k: int = 5
     ) -> Dict[str, Any]:
         """
         Feature ablation study.
-        
+
         Args:
             model_ref: Reference to trained model
             X_val_ref: Validation features reference
             y_val_ref: Validation target reference
             top_k: Number of top features to ablate
-            
+
         Returns:
             Feature importance and ablation results
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_val = load_pickle(X_val_ref)
         y_val = load_pickle(y_val_ref)
-        
+
         # Get baseline performance
         y_pred_baseline = model.predict(X_val)
-        
+
         # Calculate baseline metric
         if len(np.unique(y_val)) <= 20:  # Classification
             from sklearn.metrics import accuracy_score
+
             baseline_score = accuracy_score(y_val, y_pred_baseline)
             metric_name = "accuracy"
         else:  # Regression
             from sklearn.metrics import mean_absolute_error
+
             baseline_score = mean_absolute_error(y_val, y_pred_baseline)
             metric_name = "mae"
-        
+
         # Feature importance (if available)
         feature_importance = {}
         if hasattr(model, "feature_importances_"):
             importance_scores = model.feature_importances_
-            feature_names = X_val.columns if hasattr(X_val, 'columns') else [f"feature_{i}" for i in range(X_val.shape[1])]
+            feature_names = (
+                X_val.columns
+                if hasattr(X_val, "columns")
+                else [f"feature_{i}" for i in range(X_val.shape[1])]
+            )
             feature_importance = dict(zip(feature_names, importance_scores))
-        
+
         # Ablation study
         ablation_results = []
-        
+
         # Get top features to ablate
         if feature_importance:
-            top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:top_k]
+            top_features = sorted(
+                feature_importance.items(), key=lambda x: x[1], reverse=True
+            )[:top_k]
         else:
             # Use all features if no importance available
-            feature_names = X_val.columns if hasattr(X_val, 'columns') else [f"feature_{i}" for i in range(X_val.shape[1])]
+            feature_names = (
+                X_val.columns
+                if hasattr(X_val, "columns")
+                else [f"feature_{i}" for i in range(X_val.shape[1])]
+            )
             top_features = [(name, 1.0) for name in feature_names[:top_k]]
-        
+
         for feature_name, importance in top_features:
             # Create ablated dataset (set feature to mean/mode)
             X_ablated = X_val.copy()
-            
-            if hasattr(X_val, 'columns'):
+
+            if hasattr(X_val, "columns"):
                 col_idx = list(X_val.columns).index(feature_name)
             else:
-                col_idx = int(feature_name.split('_')[1]) if 'feature_' in feature_name else 0
-            
+                col_idx = (
+                    int(feature_name.split("_")[1]) if "feature_" in feature_name else 0
+                )
+
             # Replace with mean for numeric, mode for categorical
-            if hasattr(X_ablated, 'iloc'):
+            if hasattr(X_ablated, "iloc"):
                 col_data = X_ablated.iloc[:, col_idx]
                 if pd.api.types.is_numeric_dtype(col_data):
                     replacement_value = col_data.mean()
                 else:
-                    replacement_value = col_data.mode().iloc[0] if len(col_data.mode()) > 0 else col_data.iloc[0]
+                    replacement_value = (
+                        col_data.mode().iloc[0]
+                        if len(col_data.mode()) > 0
+                        else col_data.iloc[0]
+                    )
                 X_ablated.iloc[:, col_idx] = replacement_value
             else:
                 # NumPy array
                 col_data = X_ablated[:, col_idx]
                 replacement_value = np.mean(col_data)
                 X_ablated[:, col_idx] = replacement_value
-            
+
             # Get predictions with ablated feature
             y_pred_ablated = model.predict(X_ablated)
-            
+
             # Calculate performance drop
             if metric_name == "accuracy":
                 ablated_score = accuracy_score(y_val, y_pred_ablated)
                 performance_drop = baseline_score - ablated_score
             else:  # MAE
                 ablated_score = mean_absolute_error(y_val, y_pred_ablated)
                 performance_drop = ablated_score - baseline_score  # Higher MAE is worse
-            
-            ablation_results.append({
-                "feature": feature_name,
-                "importance": float(importance),
-                "baseline_score": float(baseline_score),
-                "ablated_score": float(ablated_score),
-                "performance_drop": float(performance_drop)
-            })
-        
+
+            ablation_results.append(
+                {
+                    "feature": feature_name,
+                    "importance": float(importance),
+                    "baseline_score": float(baseline_score),
+                    "ablated_score": float(ablated_score),
+                    "performance_drop": float(performance_drop),
+                }
+            )
+
         return {
             "baseline_score": float(baseline_score),
             "metric_name": metric_name,
             "feature_importance": {k: float(v) for k, v in feature_importance.items()},
             "ablation_results": ablation_results,
             "summary": {
-                "max_performance_drop": float(max([r["performance_drop"] for r in ablation_results], default=0)),
-                "avg_performance_drop": float(np.mean([r["performance_drop"] for r in ablation_results])) if ablation_results else 0
-            }
+                "max_performance_drop": float(
+                    max([r["performance_drop"] for r in ablation_results], default=0)
+                ),
+                "avg_performance_drop": (
+                    float(np.mean([r["performance_drop"] for r in ablation_results]))
+                    if ablation_results
+                    else 0
+                ),
+            },
         }
-    
+
     def shock_tests(
-        self,
-        model_ref: str,
-        X_val_ref: str,
-        y_val_ref: str
+        self, model_ref: str, X_val_ref: str, y_val_ref: str
     ) -> Dict[str, Any]:
         """
         Robustness shock tests.
-        
+
         Args:
             model_ref: Reference to trained model
             X_val_ref: Validation features reference
             y_val_ref: Validation target reference
-            
+
         Returns:
             Robustness grade and test results
         """
         # Load model and data
         model = load_pickle(model_ref)
         X_val = load_pickle(X_val_ref)
         y_val = load_pickle(y_val_ref)
-        
+
         # Baseline performance
         y_pred_baseline = model.predict(X_val)
-        
+
         if len(np.unique(y_val)) <= 20:  # Classification
             from sklearn.metrics import accuracy_score
+
             baseline_score = accuracy_score(y_val, y_pred_baseline)
         else:  # Regression
             from sklearn.metrics import mean_absolute_error
+
             baseline_score = mean_absolute_error(y_val, y_pred_baseline)
-        
+
         shock_results = []
-        
+
         # Test 1: Missing values shock
         X_missing = X_val.copy()
-        if hasattr(X_missing, 'iloc'):
+        if hasattr(X_missing, "iloc"):
             # Randomly set 10% of values to NaN
             mask = np.random.random(X_missing.shape) < 0.1
             X_missing = X_missing.mask(mask)
             X_missing = X_missing.fillna(X_missing.mean())  # Simple imputation
-        
+
         try:
             y_pred_missing = model.predict(X_missing)
             if len(np.unique(y_val)) <= 20:
                 missing_score = accuracy_score(y_val, y_pred_missing)
                 missing_drop = baseline_score - missing_score
             else:
                 missing_score = mean_absolute_error(y_val, y_pred_missing)
                 missing_drop = missing_score - baseline_score
-            
-            shock_results.append({
-                "test": "missing_values",
-                "performance_drop": float(missing_drop),
-                "passed": missing_drop < 0.1
-            })
+
+            shock_results.append(
+                {
+                    "test": "missing_values",
+                    "performance_drop": float(missing_drop),
+                    "passed": missing_drop < 0.1,
+                }
+            )
         except Exception:
-            shock_results.append({
-                "test": "missing_values",
-                "performance_drop": float('inf'),
-                "passed": False
-            })
-        
+            shock_results.append(
+                {
+                    "test": "missing_values",
+                    "performance_drop": float("inf"),
+                    "passed": False,
+                }
+            )
+
         # Test 2: Noise injection
-        if hasattr(X_val, 'select_dtypes'):
+        if hasattr(X_val, "select_dtypes"):
             numeric_cols = X_val.select_dtypes(include=[np.number]).columns
             if len(numeric_cols) > 0:
                 X_noisy = X_val.copy()
                 for col in numeric_cols:
                     noise = np.random.normal(0, X_val[col].std() * 0.1, len(X_val))
                     X_noisy[col] = X_val[col] + noise
-                
+
                 try:
                     y_pred_noisy = model.predict(X_noisy)
                     if len(np.unique(y_val)) <= 20:
                         noisy_score = accuracy_score(y_val, y_pred_noisy)
                         noisy_drop = baseline_score - noisy_score
                     else:
                         noisy_score = mean_absolute_error(y_val, y_pred_noisy)
                         noisy_drop = noisy_score - baseline_score
-                    
-                    shock_results.append({
-                        "test": "noise_injection",
-                        "performance_drop": float(noisy_drop),
-                        "passed": noisy_drop < 0.05
-                    })
+
+                    shock_results.append(
+                        {
+                            "test": "noise_injection",
+                            "performance_drop": float(noisy_drop),
+                            "passed": noisy_drop < 0.05,
+                        }
+                    )
                 except Exception:
-                    shock_results.append({
-                        "test": "noise_injection",
-                        "performance_drop": float('inf'),
-                        "passed": False
-                    })
-        
+                    shock_results.append(
+                        {
+                            "test": "noise_injection",
+                            "performance_drop": float("inf"),
+                            "passed": False,
+                        }
+                    )
+
         # Calculate robustness grade
         passed_tests = sum(1 for test in shock_results if test["passed"])
         total_tests = len(shock_results)
-        
+
         if total_tests == 0:
             grade = "C"
         elif passed_tests == total_tests:
             grade = "A"
         elif passed_tests >= total_tests * 0.75:
             grade = "B"
         elif passed_tests >= total_tests * 0.5:
             grade = "C"
         else:
             grade = "D"
-        
+
         return {
             "resilience_grade": grade,
             "baseline_score": float(baseline_score),
             "shock_results": shock_results,
             "summary": {
                 "passed_tests": passed_tests,
                 "total_tests": total_tests,
-                "pass_rate": float(passed_tests / total_tests) if total_tests > 0 else 0
-            }
+                "pass_rate": (
+                    float(passed_tests / total_tests) if total_tests > 0 else 0
+                ),
+            },
         }
-    
+
     def get_function_definitions(self) -> List[Dict[str, Any]]:
         """Get OpenAI function definitions."""
         return [
             {
                 "type": "function",
@@ -555,28 +594,28 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation features"
+                                "description": "Reference to validation features",
                             },
                             "y_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation target"
+                                "description": "Reference to validation target",
                             },
                             "top_k": {
                                 "type": "integer",
-                                "description": "Number of top features to ablate (default: 5)"
-                            }
+                                "description": "Number of top features to ablate (default: 5)",
+                            },
                         },
-                        "required": ["model_ref", "X_val_ref", "y_val_ref"]
-                    }
-                }
+                        "required": ["model_ref", "X_val_ref", "y_val_ref"],
+                    },
+                },
             },
             {
                 "type": "function",
                 "function": {
                     "name": "Robustness_shock_tests",
@@ -584,21 +623,21 @@
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "model_ref": {
                                 "type": "string",
-                                "description": "Reference to trained model"
+                                "description": "Reference to trained model",
                             },
                             "X_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation features"
+                                "description": "Reference to validation features",
                             },
                             "y_val_ref": {
                                 "type": "string",
-                                "description": "Reference to validation target"
-                            }
+                                "description": "Reference to validation target",
+                            },
                         },
-                        "required": ["model_ref", "X_val_ref", "y_val_ref"]
-                    }
-                }
-            }
+                        "required": ["model_ref", "X_val_ref", "y_val_ref"],
+                    },
+                },
+            },
         ]
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/botds/tools/eval.py
--- /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_acceptance_suite.py	2025-09-05 00:06:03.168265+00:00
+++ /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_acceptance_suite.py	2025-10-17 00:12:38.231125+00:00
@@ -13,251 +13,249 @@
 from botds import Config, Pipeline
 
 
 class TestAcceptanceSuite(unittest.TestCase):
     """Comprehensive acceptance tests."""
-    
+
     def setUp(self):
         """Set up test environment."""
         if not os.getenv("OPENAI_API_KEY"):
             self.skipTest("OPENAI_API_KEY not set")
-        
+
         self.temp_dir = tempfile.mkdtemp()
-    
+
     def test_determinism_across_runs(self):
         """Test that re-running produces identical results within CI overlap."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
         config.report.out_dir = self.temp_dir
-        
+
         # Run 1
         pipeline1 = Pipeline(config)
         result1 = pipeline1.run()
         self.assertEqual(result1["status"], "success")
-        
+
         # Run 2 with same config
         pipeline2 = Pipeline(config)
         result2 = pipeline2.run()
         self.assertEqual(result2["status"], "success")
-        
+
         # Load split indices from both runs
         artifacts1 = Path(result1["artifacts_dir"])
         artifacts2 = Path(result2["artifacts_dir"])
-        
+
         splits1_path = artifacts1 / "handoffs" / "split_indices.json"
         splits2_path = artifacts2 / "handoffs" / "split_indices.json"
-        
+
         if splits1_path.exists() and splits2_path.exists():
             with open(splits1_path) as f:
                 splits1 = json.load(f)
             with open(splits2_path) as f:
                 splits2 = json.load(f)
-            
+
             # Split indices should be identical (same seed)
             self.assertEqual(splits1["indices"], splits2["indices"])
-    
+
     def test_budget_enforcement(self):
         """Test budget limits trigger appropriate responses."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
         config.report.out_dir = self.temp_dir
-        
+
         # Set very tight time budget
         config.budgets.time_min = 1  # 1 minute
-        
-        pipeline = Pipeline(config)
-        
+
+        pipeline = Pipeline(config)
+
         # Should either complete with shortcuts or fail gracefully
         result = pipeline.run()
-        
+
         # Check that budget monitoring occurred
         if result["status"] == "success":
             # If successful, check for shortcuts in manifest
             manifest_path = Path(result["artifacts_dir"]) / "run_manifest.json"
             if manifest_path.exists():
                 with open(manifest_path) as f:
                     manifest = json.load(f)
-                
+
                 # May have shortcuts due to tight budget
                 shortcuts = manifest.get("shortcuts_taken", [])
                 # This is acceptable - tight budget may require shortcuts
         else:
             # If failed, should be due to budget constraints
             self.assertIn("budget", result.get("error", "").lower())
-    
+
     def test_authority_enforcement(self):
         """Test that OpenAI is required for critical decisions."""
         # This test verifies the system fails without OpenAI key
         original_key = os.environ.get("OPENAI_API_KEY")
-        
+
         try:
             # Remove OpenAI key
             if "OPENAI_API_KEY" in os.environ:
                 del os.environ["OPENAI_API_KEY"]
-            
+
             config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
-            
+
             # Should fail during config validation
             with self.assertRaises(ValueError) as context:
                 config = Config.from_yaml(str(config_path))
                 config.validate_environment()
-            
+
             self.assertIn("OPENAI_API_KEY", str(context.exception))
             self.assertIn("sole decision authority", str(context.exception))
-            
+
         finally:
             # Restore original key
             if original_key:
                 os.environ["OPENAI_API_KEY"] = original_key
-    
+
     def test_decision_log_completeness(self):
         """Test that all critical decisions are logged with OpenAI authority."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
         config.report.out_dir = self.temp_dir
-        
-        pipeline = Pipeline(config)
-        result = pipeline.run()
-        self.assertEqual(result["status"], "success")
-        
+
+        pipeline = Pipeline(config)
+        result = pipeline.run()
+        self.assertEqual(result["status"], "success")
+
         # Check decision log exists and has entries
-        decision_log_path = Path(result["artifacts_dir"]) / "logs" / "decision_log.jsonl"
+        decision_log_path = (
+            Path(result["artifacts_dir"]) / "logs" / "decision_log.jsonl"
+        )
         self.assertTrue(decision_log_path.exists())
-        
+
         # Read decision log
         decisions = []
         with open(decision_log_path) as f:
             for line in f:
                 if line.strip():
                     decisions.append(json.loads(line))
-        
+
         # Should have multiple critical decisions
         self.assertGreater(len(decisions), 0)
-        
+
         # All decisions should be from OpenAI
         for decision in decisions:
             self.assertEqual(decision.get("auth_model"), "openai")
             self.assertIn("stage", decision)
             self.assertIn("decision", decision)
             self.assertIn("timestamp", decision)
-    
+
     def test_handoff_traceability(self):
         """Test that all handoffs are logged with proper schemas."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
         config.report.out_dir = self.temp_dir
-        
-        pipeline = Pipeline(config)
-        result = pipeline.run()
-        self.assertEqual(result["status"], "success")
-        
+
+        pipeline = Pipeline(config)
+        result = pipeline.run()
+        self.assertEqual(result["status"], "success")
+
         # Check handoff ledger
         ledger_path = Path(result["artifacts_dir"]) / "logs" / "handoff_ledger.jsonl"
-        
+
         if ledger_path.exists():
             handoffs = []
             with open(ledger_path) as f:
                 for line in f:
                     if line.strip():
                         handoffs.append(json.loads(line))
-            
+
             # Should have handoff entries
             self.assertGreater(len(handoffs), 0)
-            
+
             # Each handoff should have required fields
             for handoff in handoffs:
                 self.assertIn("stage", handoff)
                 self.assertIn("inputs", handoff)
                 self.assertIn("outputs", handoff)
                 self.assertIn("schema", handoff)
                 self.assertIn("hash", handoff)
                 self.assertIn("timestamp", handoff)
-    
+
     def test_schema_validation(self):
         """Test that handoff files match their schemas."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
         config.report.out_dir = self.temp_dir
-        
-        pipeline = Pipeline(config)
-        result = pipeline.run()
-        self.assertEqual(result["status"], "success")
-        
+
+        pipeline = Pipeline(config)
+        result = pipeline.run()
+        self.assertEqual(result["status"], "success")
+
         artifacts_dir = Path(result["artifacts_dir"])
         handoffs_dir = artifacts_dir / "handoffs"
-        
+
         # Check that key handoff files exist and are valid JSON
-        expected_handoffs = [
-            "profile.json",
-            "split_indices.json",
-            "feature_plan.json"
-        ]
-        
+        expected_handoffs = ["profile.json", "split_indices.json", "feature_plan.json"]
+
         for handoff_file in expected_handoffs:
             handoff_path = handoffs_dir / handoff_file
             if handoff_path.exists():
                 # Should be valid JSON
                 with open(handoff_path) as f:
                     data = json.load(f)
-                
+
                 # Should have some content
                 self.assertIsInstance(data, dict)
                 self.assertGreater(len(data), 0)
-    
+
     def test_report_completeness(self):
         """Test that reports contain all required sections."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
         config.report.out_dir = self.temp_dir
-        
-        pipeline = Pipeline(config)
-        result = pipeline.run()
-        self.assertEqual(result["status"], "success")
-        
+
+        pipeline = Pipeline(config)
+        result = pipeline.run()
+        self.assertEqual(result["status"], "success")
+
         # Check one-pager exists and has content
         one_pager_path = Path(result["artifacts_dir"]) / "reports" / "one_pager.html"
         self.assertTrue(one_pager_path.exists())
-        
+
         with open(one_pager_path) as f:
             content = f.read()
-        
+
         # Check for required sections
         required_sections = [
             "Problem & Success Metric",
-            "Data Snapshot", 
+            "Data Snapshot",
             "Top 3 Insights",
             "Model Decision",
             "Operating Point",
             "Robustness Grade",
-            "Next Steps"
+            "Next Steps",
         ]
-        
+
         for section in required_sections:
             self.assertIn(section, content, f"Missing section: {section}")
-    
+
     def test_reproducibility_manifest(self):
         """Test that run manifest contains reproducibility information."""
         config_path = Path(__file__).parent.parent / "configs" / "iris.yaml"
         config = Config.from_yaml(str(config_path))
         config.report.out_dir = self.temp_dir
-        
-        pipeline = Pipeline(config)
-        result = pipeline.run()
-        self.assertEqual(result["status"], "success")
-        
+
+        pipeline = Pipeline(config)
+        result = pipeline.run()
+        self.assertEqual(result["status"], "success")
+
         # Check manifest exists
         manifest_path = Path(result["artifacts_dir"]) / "run_manifest.json"
         self.assertTrue(manifest_path.exists())
-        
+
         with open(manifest_path) as f:
             manifest = json.load(f)
-        
+
         # Check required fields for reproducibility
         self.assertIn("job_id", manifest)
         self.assertIn("created_at", manifest)
         self.assertIn("seeds", manifest)
-        
+
         # Job ID should be 8 characters
         self.assertEqual(len(manifest["job_id"]), 8)
 
 
 if __name__ == "__main__":
would reformat /Users/sid47/Documents/augment-projects/Data Scientist/tests/test_acceptance_suite.py

Oh no! 💥 💔 💥
21 files would be reformatted, 2 files would be left unchanged.
